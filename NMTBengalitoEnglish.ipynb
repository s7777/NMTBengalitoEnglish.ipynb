{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffgXy6av6sMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cac9499-8347-4ecc-8b97-b3f751275311"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\t#table = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\t#line = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/ben.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-german.pkl\n",
            "[go] => [যাও।]\n",
            "[go] => [যান।]\n",
            "[go] => [যা।]\n",
            "[run] => [পালাও]\n",
            "[run] => [পালান]\n",
            "[who] => [কে]\n",
            "[fire] => [আগুন]\n",
            "[help] => [বাঁচাও]\n",
            "[help] => [বাঁচান]\n",
            "[stop] => [থামুন]\n",
            "[stop] => [থামো]\n",
            "[stop] => [থাম]\n",
            "[hello] => [নমস্কার]\n",
            "[i see] => [বুঝলাম।]\n",
            "[i try] => [আমি চেষ্টা করি।]\n",
            "[smile] => [একটু হাসুন।]\n",
            "[smile] => [একটু হাসো।]\n",
            "[attack] => [আক্রমণ]\n",
            "[get up] => [ওঠো।]\n",
            "[get up] => [উঠুন।]\n",
            "[got it] => [বুঝে গেছি]\n",
            "[got it] => [ধরেছি]\n",
            "[got it] => [বুঝেছো]\n",
            "[got it] => [বুঝেছেন]\n",
            "[got it] => [বুঝেছিস]\n",
            "[i know] => [আমি জানি।]\n",
            "[i know] => [আমার জানা আছে।]\n",
            "[i lost] => [আমি হেরে গেছি।]\n",
            "[im 19] => [আমার ১৯ বছর বয়স।]\n",
            "[im ok] => [আমি ঠিক আছি।]\n",
            "[listen] => [শোন।]\n",
            "[listen] => [শুনুন।]\n",
            "[no way] => [কোন মতেই না]\n",
            "[really] => [সত্যি]\n",
            "[really] => [তাই নাক]\n",
            "[thanks] => [ধন্যবাদ।]\n",
            "[try it] => [চেখে দেখুন।]\n",
            "[try it] => [চেখে দেখো।]\n",
            "[we won] => [আমরা জিতে গেছে।]\n",
            "[why me] => [আমিই কেন]\n",
            "[ask tom] => [টমকে জিজ্ঞাসা করো।]\n",
            "[ask tom] => [টমকে জিজ্ঞাসা করুন।]\n",
            "[ask tom] => [টমকে জিজ্ঞাসা কর।]\n",
            "[call me] => [আমাকে ডেক।]\n",
            "[call me] => [আমাকে ডাকবেন।]\n",
            "[call me] => [আমাকে ডাকিস।]\n",
            "[call us] => [আমাদের ডেকো।]\n",
            "[call us] => [আমাদের ডাকিস।]\n",
            "[call us] => [আমাদের ডাকবেন।]\n",
            "[come in] => [ভেতরে আসুন।]\n",
            "[come in] => [ভেতরে আসো।]\n",
            "[get tom] => [টমকে আনো।]\n",
            "[get tom] => [টমকে ডাকো।]\n",
            "[get tom] => [টমকে ধরো।]\n",
            "[get tom] => [টমকে আনুন।]\n",
            "[get tom] => [টমকে ডাকুন।]\n",
            "[get tom] => [টমকে ধরুন।]\n",
            "[get out] => [বেরও।]\n",
            "[get out] => [বেরো।]\n",
            "[go away] => [চলে যাও]\n",
            "[go away] => [চলে যান।]\n",
            "[go away] => [চলে যাও।]\n",
            "[go away] => [চলে যান।]\n",
            "[go home] => [বাড়ি যাও।]\n",
            "[go home] => [বাড়ি যা।]\n",
            "[go home] => [বাড়ি যান।]\n",
            "[go slow] => [আস্তে যান।]\n",
            "[go slow] => [আস্তে যাও।]\n",
            "[go slow] => [আস্তে যা।]\n",
            "[goodbye] => [বিদায়।]\n",
            "[goodbye] => [এলাম]\n",
            "[he came] => [সে এসেছিলো।]\n",
            "[he came] => [উনি এসেছিলেন।]\n",
            "[he came] => [ও এসেছিলো।]\n",
            "[he came] => [তিনি এসেছিলেন।]\n",
            "[he runs] => [এ দৌড়ায়।]\n",
            "[he runs] => [ইনি দৌড়ান।]\n",
            "[he runs] => [ও দৌড়ায়।]\n",
            "[he runs] => [সে দৌড়ায়।]\n",
            "[he runs] => [উনি দৌড়ান।]\n",
            "[he runs] => [তিনি দৌড়ান।]\n",
            "[help me] => [আমাকে সাহায্য করুন]\n",
            "[help us] => [আমাদের সাহায্য করুন।]\n",
            "[help us] => [আমাদের সাহায্য করো।]\n",
            "[help us] => [আমাদের সাহায্য কর।]\n",
            "[hit tom] => [টমকে মারো।]\n",
            "[hit tom] => [টমকে মার।]\n",
            "[hit tom] => [টমকে মারুন।]\n",
            "[i agree] => [আমি একমত।]\n",
            "[i tried] => [আমি চেষ্টা করেছিলাম।]\n",
            "[i tried] => [আমি চেষ্টা করেছি।]\n",
            "[im tom] => [আমি টম।]\n",
            "[im shy] => [আমি লাজুক।]\n",
            "[im wet] => [আমি ভিজে গেছি।]\n",
            "[its me] => [আমি।]\n",
            "[keep it] => [ওটা রাখো।]\n",
            "[keep it] => [ওটা রাখুন।]\n",
            "[keep it] => [ওটা রাখ।]\n",
            "[me too] => [আমিও।]\n",
            "[perfect] => [একদম ঠিক]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9KcTXYm3W9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "turwA4Mk7aoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28f96ac-680b-4010-931b-03a977d73fb6"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-g-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8JMmeH_7kwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc01cc4e-9617-41ba-c0f5-b9c5e5c88a40"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1875\n",
            "English Max Length: 19\n",
            "German Vocabulary Size: 3310\n",
            "German Max Length: 18\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 18, 256)           847360    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 19, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 19, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 19, 1875)          481875    \n",
            "=================================================================\n",
            "Total params: 2,379,859\n",
            "Trainable params: 2,379,859\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.81009, saving model to model.h5\n",
            "68/68 - 33s - loss: 2.8035 - val_loss: 1.8101\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.81009 to 1.60016, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.6904 - val_loss: 1.6002\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.60016 to 1.51209, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.5745 - val_loss: 1.5121\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.51209 to 1.44415, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.4857 - val_loss: 1.4441\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.44415 to 1.41067, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.4407 - val_loss: 1.4107\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.41067 to 1.40187, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.4131 - val_loss: 1.4019\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.40187 to 1.36144, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.3922 - val_loss: 1.3614\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.36144 to 1.34147, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.3632 - val_loss: 1.3415\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.34147 to 1.33058, saving model to model.h5\n",
            "68/68 - 34s - loss: 1.3495 - val_loss: 1.3306\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.33058 to 1.31253, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.3390 - val_loss: 1.3125\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.31253 to 1.29530, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.3161 - val_loss: 1.2953\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.29530 to 1.27138, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.2939 - val_loss: 1.2714\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.27138 to 1.25071, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.2759 - val_loss: 1.2507\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.25071 to 1.23530, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.2610 - val_loss: 1.2353\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.23530 to 1.21890, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.2466 - val_loss: 1.2189\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.21890 to 1.19801, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.2274 - val_loss: 1.1980\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.19801 to 1.17908, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.2072 - val_loss: 1.1791\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.17908 to 1.15247, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.1850 - val_loss: 1.1525\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.15247 to 1.13244, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.1614 - val_loss: 1.1324\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.13244 to 1.11186, saving model to model.h5\n",
            "68/68 - 33s - loss: 1.1405 - val_loss: 1.1119\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.11186 to 1.10008, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.1283 - val_loss: 1.1001\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.10008 to 1.06896, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.1073 - val_loss: 1.0690\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.06896 to 1.04886, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.0879 - val_loss: 1.0489\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.04886 to 1.02414, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.0628 - val_loss: 1.0241\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.02414 to 0.99462, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.0313 - val_loss: 0.9946\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.99462 to 0.96224, saving model to model.h5\n",
            "68/68 - 32s - loss: 1.0005 - val_loss: 0.9622\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.96224 to 0.93762, saving model to model.h5\n",
            "68/68 - 35s - loss: 0.9729 - val_loss: 0.9376\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.93762 to 0.90537, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.9455 - val_loss: 0.9054\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.90537 to 0.87415, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.9145 - val_loss: 0.8741\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.87415 to 0.85255, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.8874 - val_loss: 0.8526\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.85255 to 0.81774, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.8597 - val_loss: 0.8177\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.81774 to 0.78867, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.8309 - val_loss: 0.7887\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.78867 to 0.76092, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.8006 - val_loss: 0.7609\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.76092 to 0.73254, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.7723 - val_loss: 0.7325\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.73254 to 0.70656, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.7473 - val_loss: 0.7066\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.70656 to 0.67922, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.7209 - val_loss: 0.6792\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.67922 to 0.65611, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.6946 - val_loss: 0.6561\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.65611 to 0.62865, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.6684 - val_loss: 0.6287\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.62865 to 0.60202, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.6410 - val_loss: 0.6020\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.60202 to 0.59507, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.6197 - val_loss: 0.5951\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.59507 to 0.56100, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.5994 - val_loss: 0.5610\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.56100 to 0.53642, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.5757 - val_loss: 0.5364\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.53642 to 0.51856, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.5515 - val_loss: 0.5186\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.51856 to 0.49411, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.5293 - val_loss: 0.4941\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.49411 to 0.47267, saving model to model.h5\n",
            "68/68 - 35s - loss: 0.5071 - val_loss: 0.4727\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.47267 to 0.45336, saving model to model.h5\n",
            "68/68 - 34s - loss: 0.4861 - val_loss: 0.4534\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.45336 to 0.44048, saving model to model.h5\n",
            "68/68 - 34s - loss: 0.4680 - val_loss: 0.4405\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.44048 to 0.42505, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.4507 - val_loss: 0.4250\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.42505 to 0.40729, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.4364 - val_loss: 0.4073\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.40729 to 0.40178, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.4217 - val_loss: 0.4018\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.40178 to 0.37021, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.4025 - val_loss: 0.3702\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.37021 to 0.35357, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.3823 - val_loss: 0.3536\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.35357 to 0.33852, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.3659 - val_loss: 0.3385\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.33852 to 0.32580, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.3507 - val_loss: 0.3258\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.32580 to 0.31296, saving model to model.h5\n",
            "68/68 - 33s - loss: 0.3380 - val_loss: 0.3130\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.31296 to 0.29689, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.3218 - val_loss: 0.2969\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.29689\n",
            "68/68 - 33s - loss: 0.3100 - val_loss: 0.2979\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.29689 to 0.27318, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2998 - val_loss: 0.2732\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.27318 to 0.26324, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.2855 - val_loss: 0.2632\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.26324 to 0.25480, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.2750 - val_loss: 0.2548\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.25480 to 0.25170, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2694 - val_loss: 0.2517\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.25170 to 0.24076, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2576 - val_loss: 0.2408\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.24076 to 0.22712, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.2451 - val_loss: 0.2271\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.22712 to 0.21119, saving model to model.h5\n",
            "68/68 - 35s - loss: 0.2342 - val_loss: 0.2112\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.21119 to 0.20020, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2207 - val_loss: 0.2002\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.20020 to 0.19804, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2132 - val_loss: 0.1980\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.19804 to 0.18876, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2066 - val_loss: 0.1888\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.18876 to 0.18256, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.2002 - val_loss: 0.1826\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.18256 to 0.17861, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1942 - val_loss: 0.1786\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.17861 to 0.16944, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1887 - val_loss: 0.1694\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.16944 to 0.16163, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1780 - val_loss: 0.1616\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.16163 to 0.15408, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1689 - val_loss: 0.1541\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.15408 to 0.14715, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.1608 - val_loss: 0.1472\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.14715 to 0.14428, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1560 - val_loss: 0.1443\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.14428 to 0.14142, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.1558 - val_loss: 0.1414\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.14142 to 0.13532, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1499 - val_loss: 0.1353\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.13532\n",
            "68/68 - 32s - loss: 0.1424 - val_loss: 0.1377\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.13532 to 0.12632, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1419 - val_loss: 0.1263\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.12632 to 0.12038, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1349 - val_loss: 0.1204\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.12038 to 0.11623, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1278 - val_loss: 0.1162\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.11623 to 0.11048, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1209 - val_loss: 0.1105\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.11048\n",
            "68/68 - 32s - loss: 0.1165 - val_loss: 0.1157\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.11048 to 0.10546, saving model to model.h5\n",
            "68/68 - 35s - loss: 0.1146 - val_loss: 0.1055\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.10546 to 0.09947, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1109 - val_loss: 0.0995\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.09947 to 0.09909, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.1070 - val_loss: 0.0991\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.09909 to 0.09348, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.1046 - val_loss: 0.0935\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.09348\n",
            "68/68 - 31s - loss: 0.1019 - val_loss: 0.0980\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.09348\n",
            "68/68 - 31s - loss: 0.1036 - val_loss: 0.0982\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.09348 to 0.08991, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.0990 - val_loss: 0.0899\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.08991 to 0.08614, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.0963 - val_loss: 0.0861\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.08614 to 0.08595, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.0945 - val_loss: 0.0859\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.08595 to 0.07717, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.0894 - val_loss: 0.0772\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.07717 to 0.07430, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.0837 - val_loss: 0.0743\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.07430 to 0.07283, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.0800 - val_loss: 0.0728\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.07283 to 0.06964, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.0775 - val_loss: 0.0696\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.06964 to 0.06686, saving model to model.h5\n",
            "68/68 - 32s - loss: 0.0762 - val_loss: 0.0669\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.06686\n",
            "68/68 - 31s - loss: 0.0751 - val_loss: 0.0673\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.06686 to 0.06440, saving model to model.h5\n",
            "68/68 - 31s - loss: 0.0719 - val_loss: 0.0644\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.06440\n",
            "68/68 - 31s - loss: 0.0699 - val_loss: 0.0654\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.06440\n",
            "68/68 - 31s - loss: 0.0704 - val_loss: 0.0674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f945e3994e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriO0ereAdDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913ade5d-e9de-4960-be2b-f71193f76433"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[উনি কথা বলছিলেন।], target=[he was speaking], predicted=[he was speaking]\n",
            "src=[ওরা পরোয়া করে না।], target=[they dont care], predicted=[they dont care]\n",
            "src=[টম মেরিকে হুমকি দিলো।], target=[tom threatened mary], predicted=[tom threatened mary]\n",
            "src=[ফুলদানিটা কে ভাঙল], target=[who broke the vase], predicted=[who broke the vase]\n",
            "src=[টম বারে দাঁড়িয়ে বিয়ার খাচ্ছিল।], target=[tom was standing at the bar having a beer], predicted=[tom was standing at the bar drinking a beer]\n",
            "src=[আমি বেকারিতে গেলাম।], target=[i went to the bakery], predicted=[i went to the bakery]\n",
            "src=[তুমি কি মাংস খাও], target=[do you eat meat], predicted=[do you eat meat]\n",
            "src=[আরও চেষ্টা কর।], target=[try hard], predicted=[try hard]\n",
            "src=[আমি ঠিক বুঝতে পরছি না।], target=[i do not understand], predicted=[i dont get it]\n",
            "src=[ওকে ফ্যাকাসে দেখাচ্ছে।], target=[he looks pale], predicted=[he looks pale]\n",
            "BLEU-1: 0.946882\n",
            "BLEU-2: 0.923017\n",
            "BLEU-3: 0.893905\n",
            "BLEU-4: 0.822678\n",
            "test\n",
            "src=[আপনি কে], target=[who are you], predicted=[who are you]\n",
            "src=[আমি ফিরে আসছি।], target=[im coming back], predicted=[im coming back]\n",
            "src=[আমি ভেবেছিলাম এটা করা সহজ হবে কিন্তু আমরা সারাদিন ধরে কাজ করেছি আর এখনো শেষ করে উঠতে পারিনি।], target=[i thought doing this would be easy but weve been working all day and were still not finished], predicted=[i thought doing it it be easy but weve weve all all all all were still still finished]\n",
            "src=[টম প্রায় প্রত্যেক দিন কেঁদেছে।], target=[tom has been crying almost every day], predicted=[tom has been crying almost every day]\n",
            "src=[তোমার কাছে একটা পেন হবে], target=[do you have a pen on you], predicted=[do you have a pen on you]\n",
            "src=[আমার কাছে মাত্র একটাই কম্বল আছে।], target=[i only have one blanket], predicted=[i only have one blanket]\n",
            "src=[আমি একটা অ্যাকাউন্ট খুলতে চাই।], target=[id like to open an account], predicted=[id like to open an account]\n",
            "src=[আমি আপনার উপর নির্ভর করে আছি।], target=[i am counting on you], predicted=[i counting counting on]\n",
            "src=[আমি তোমাকে চেঁচাতে শুনলাম।], target=[i heard you scream], predicted=[i heard you screaming]\n",
            "src=[তুমি চিৎকার করছো কেন], target=[why are you shouting], predicted=[why are you yelling]\n",
            "BLEU-1: 0.948236\n",
            "BLEU-2: 0.924976\n",
            "BLEU-3: 0.895370\n",
            "BLEU-4: 0.825149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7s4ziPaX8EM"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-bangla-both.pkl')\n",
        "train = load_clean_sentences('english-bangla-train.pkl')\n",
        "test = load_clean_sentences('english-bangla-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare Bengali tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BKAQxxYKOji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "67cd2d2f-8fc0-49f3-d9d3-5f41953cc868"
      },
      "source": [
        "# load data\n",
        "filename = '/content/ct.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "# stemming of words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:2000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeff', '1.আঁই', 'ন', 'গইজ্জুম', '2.তোঁয়ারা', 'গম', 'আছনে', '?', '৩।', 'অঁনে', 'গম', 'আছন্নে', '?', '৪।অনরা', 'কেন', 'আছন', '?', '৫।ইতারা', 'কেং', 'গরের', '?', '৬/আঁর', 'বাড়ি', 'খাসসবাজার', '৭/ইন', 'ইতে', 'চাইয়েদেরী', '৮/আই', 'কেং', 'গইরত্তাম', '৯/কুঁইইরে', 'হঅরাইইয়ে', '১০/মুশা', 'হঅরার', 'দে', 'আঁরে', '১১/বিলাই', 'মাছ', 'চুরি', 'গরি', 'খাই', 'ফেলাইয়ে', 'দে', '১২/দিয়া', 'ফইল্লে', 'দি', 'ফেল', 'গই', '১৩/আঁই', 'কি', 'হইতাম', 'এরে', '১৪/ইতারা', 'গম', 'মারেদ্দে', '১৫/বিইন্না', 'ভাত', 'খাই', '১৬/দুইজ্জা', 'হাআলি', 'ইছা', 'মাছ', 'দিই', 'অরে', 'ভাত', 'খাইই', 'দেএরি', '১৭/আছরত্তত', 'আইসসুম', 'আঁই', '১৮/', 'তেইল্যাচুরা', 'লাই', 'কিছু', 'রাখিত', 'ন', 'পারির', '১৯/টুট', 'টুই', 'ঢাহেদ্দে', 'আ', '২০/হাট্টল', 'খাইওস', 'নে', '?', '২১১/ইতাত্তো', 'বাজমালর', 'দোয়ান', 'আছে', '২১/বেজ্ঞিন', 'ঘটি', 'যার', 'গই', '২২/', 'ইতে', 'বিয়া', 'গইজ্জে', '২৩/তুয়ার', 'গিন', 'তুয়ারে', 'দি', 'ফেলাই', '২৪/মধুপুক', 'অর', 'মধু', 'ইন', '২৫/', 'আঁই', 'খুইশুল', 'খাইইমু', '২৬/আইজ্জা', 'কি', 'রাইন্ধঅ', '?', '২৭/এত', 'মাছর', 'মাঝত', 'ইবা', 'কি', 'মাছ', '২৮/মাছ', 'ইবা', 'গম', 'গরি', 'ডঅর', '২৯/', 'কুরা', 'গোসস', 'খাইবা', 'নে', '?', '৩০/কুরা', 'কিন্নন', 'নেকি', '?', '৩১/কুরার', 'ছ', 'গিন', 'চো্রে', 'লই', 'গিইয়েগই', '৩২/সেমিট', 'গিন', 'হরে', 'রাইক্ক', 'তুঁই', '?', '৩৩/ছাই', 'এনা', 'ছইড', 'ন', 'গরিঅ', '৩৪/আঁরার', 'বাসাত', 'মেমান', '(', 'গরবা', ')', 'আইসসে', '৩৫/আঁই', 'ন', 'ফাইজ্জুম', '৩৬/বাআরে', 'খাঁউয়া', 'ডাহের', '৩৭।ইতারা', 'কি', 'গইজ্জেদে', '৩৮/গঅলইস', 'গিন', 'দুই', 'দিও', '৩৯/খাঁফ', 'পিরিচ', 'গিন', 'সুন্দর', 'আছে', '৪০/হইজ্জা', 'ন', 'গইজ্জ', 'ছাই', 'এক্কা', '৪১/অড়ি', 'হইজ্জা', 'ন', 'গরিস', '৪২/ইতে', 'বেক্কিন', 'জানে', '৪৩/অনে', 'বেক্কিন', 'জানন', '৪৪/এক্কানা', 'হইজ্জা', 'ন', 'গইজ্জন', 'ছাই', '৪৫/ইবা', 'বেক্কিন', 'জানে', '৪৬/তোয়ার', 'নাম', 'কি', '?', '৪৭/ইতাত্তো', 'বাজমালর', 'দোয়ান', 'আছে', '৪৮/আঁই', 'খেলাইত', 'যাইর', 'এহন', '৪৯/চগলাইট', 'খাইবা', 'নে', '?', '৫০/আঁই', 'জানি', '৫১/আঁই', 'ভাত', 'খাই', '৫২/হইতর', '(', 'কবুতর', ')', 'গিন', 'বাদি', 'রাখন', 'ছাই', '৫৩/অন্দুর', 'রে', 'বেক্কিন', 'নাশ', 'গইজ্জে', '৫৪/আঁত্তো', 'কিছু', 'গম', 'ন', 'লাগের', '৫৫/কি', 'দি', 'হাইঅন', '?', '৫৬/ভাত', 'হাইঅইনে', 'নে', '?', '৫৭/ইবা/ইতে', 'আঁকিত', 'জানে', 'ভালা', '৫৮/হাইল্লা', 'যাইইয়ুম', 'আঁই', 'শহরত', '৫৯/উগগা', 'ফুয়ারে', 'ভর', 'নে', 'ফাতল', 'তুলি', 'ন', 'ছায়', '৬০/শাট', 'ফেন', 'পরি', 'হরে', 'যঅর', '?', '৬১/মন', 'দি', 'গরিবা', '৬২/মন', 'দি', 'হাআম', 'গরিও', '৬৩/আঁই', 'অনরে', 'ন', 'চিনি', '৬৪/কিছু', 'মনত', 'ন', 'গইজ্জন', '৬৫/তুঁই', 'ঝাউরা', '৬৬/ইতে', 'মরি', 'গিইইয়েগই', '৬৭/অসুখত', 'পইজ্জে', 'দেএরি', '৬৮/৬', 'টাত', 'শেষ', 'অইয়ে', '৬৯/আঁই', 'পুছার', 'গইজ্জুম', '৭০/আজার', 'আজার', 'মানুষ', 'অর', 'ঢল', 'নাইম্মে', '৭১/কেং', 'গইজ্জ', 'দে', '৭২/ডর', 'পুয়া', ',', 'মাইজ্জা', 'পোয়া', ',', 'গুরা', 'পোয়া', 'মিলি', 'কেন', 'গরর', '৭৩/ইতার', 'মাইজ্জা', 'যেউরফা', 'গা', 'ভালা', '৭৪/আঁই', 'ইতারে', 'গম', 'ফাই', '৭৫/আতিক্কা', 'মিক্কান', 'ডরাইয়ে', 'দে', '৭৬/আঁরা', 'ন', 'যাইইউম', '৭৭/এক্কু', 'পোঁয়াতি', 'যাইয়ুম', '৭৮৮/আঁই', 'কেং', 'গইত্তাম', '৭৮/অনরে', 'পোছার', 'গইজ্জিদে', 'ননে', '৭৯/আঁর', 'অসুখ', 'লাগের', '৮০/চুখত', 'ভুল', 'দেহির', '৮১/চইল', 'ফুরাই', 'গিইওই', '৮২/বারাবারি', 'ন', 'গরিঅ', '৮৩/বাআন্ডাত', 'বইঅ', 'বাতাস', 'আছে', 'এরে', '৮৪/আতিনার', 'দুয়ারত', 'বইসসে', 'মেমান', '(', 'গরবা', ')', '৮৫/দইনত', 'গরে', 'থাকে', 'দে', 'আব্বা', '৮৬/উডান', 'নান', 'এক্কা', 'ঝাড়ু', 'দ', 'গই', '৮৭/ফুরইন', 'হরে', 'রাইক্কে', 'মম্মা', '(', 'আম্মা', ')', '?', '৮৮/অউলাত', 'রাধিরদে', '৮৯/অউলাত', 'মম্মা', '(', 'আম্মা', ')', 'গোসস', 'রাঁধের', '৯০/ন', 'লড়িও', '৯১/অডি/অড়ি', 'ডর', 'লাগেদ্দে', '৯২/এক্কানা', 'আচ্ছে', 'ছালাছেনা/ছালাছুনা', '৯৩/এক্কানাইচ্ছ', 'পো্যাইন', 'এদিন', 'গা', 'শতানিগরে', 'আ', '৯৪/গা', 'ধুই', 'আঁই', ',', 'তয়', 'হতা', 'খুইয়ুম', '৯৫/কি', 'খইঅ', 'দে', '?', '৯৬/আঁর', 'তু', 'টিয়া', 'নাই', '৯৭/', 'তুঁই', 'কি', 'হঅদ্দে', '?', '৯৮/তুঁই', 'কি', 'হইবা', 'খঅ', '?', '৯৯/তুঁই', 'কি', 'গরর', 'দে', '?', '১০০/তুই', 'গম', 'আছ', 'নে', '?', '১০১/আঁই', 'যাই', 'ন', 'ফারিদ্দে', '১০২/এক্কিনিচ্চ', 'পুজলি', 'এংকেন', 'লাফাই', 'ফারে', '!', '১০৩/এবার', 'ন', 'অইব', '১০৪/আঁরা', 'ফুয়াতি', 'বই', 'থাইক্কি', '১০৫/আঁরা', 'ফুয়াতি', 'খাম', 'গরি', '১০৬/তুই', 'এন', 'গরর', 'ক্যা', '১০৭/', 'কঅত্তে', 'যাইবান', '?', '১০৮/হ', 'টিয়া', 'লইবান', '?', '১০১/অনে', 'বেশী', 'চঅন', 'দেয়া', '!', '!', '!', '!', '১০২/আরেক্কানা', 'হঅমন', '১০৩/বাআজি', 'আঁরা', 'ইনর', 'দাম', 'জানি', '১০৪/এন্ডিলা', 'কা', 'দাম', 'চাইত', 'লাইজ্ঞন', '১০৯/তুঁই', 'এন', 'গরর', 'কা', '?', '১১০/আঁই', 'ন', 'জানি', '১১১/ইতারা', 'খইজ্জা', 'গরের', '১১২/তুই', 'আঁর', 'জাউরা', 'ভাই/বইন', '১০৯।তুই', 'আঁর', 'জাউরা', 'বইন', '১১০/ইবা', 'রাইত্যা', 'কুডুর', 'কুডুর', 'গরি', 'বিস্কুট', 'খাআদ্দে', '১১৩/ইতে', 'কি', 'গরেদ্দে', '?', '১১৪/মাইয়া', 'পোয়া', 'ইবা', 'বেশী', 'টাউট', '১১৫/মরতপোয়া', 'ইতে', 'বেশী', 'টাউট', '১১৬/আঁই', 'তো্যারে', 'গম', 'ফাই', '১১৭/তুঁই', 'আঁরে', 'গম', 'ফাই', 'অ', 'দেএরী', '১১৮/মরার', 'ধইল', 'ফরি', 'কা', 'রইঅস', '?', '১১৯/ঘাট', 'ফান', 'বাধি', 'অরে', 'নাম', '১১৮/', 'ঘাট', 'ফান', 'বাধিওরে', 'নাইম্মে', 'ইতে', '১২০/', 'মাইয়া', 'অউর', 'বাড়িত', 'গিয়ে', '১২১/অড়া', 'বেশি', 'খতা', 'খস', 'তুঁই', '১২২/আঁই', 'তর', 'মশকইজ্জা', 'লাগি', 'নেকি', '?', '১২৩/মুরব্বির', 'লগে', 'শতানি', 'গরস', 'দে', 'নেকি', '১২৪/মশকারা', 'গরে', 'ইতে', 'বেশী', '১২৫/অ', 'বাজি', 'তল্লা', 'পেট', 'পুরেদ্দে', '১২৬/বাড়িত', 'আইওন', ',', 'বেরাইওত', '১২৭/খাম', 'শেস', 'গরি', 'আইসসুম', '১২৮/বেক্কিন', 'সেট', 'গরির', '১২৯/আঁর', 'লাইও', 'দোয়া', 'গইজ্জন', '১৩০/কি', 'গরন', 'দে', '?', '১৩১/কি', 'হবর', '?', '১৩২/হত্যে', 'যাইবান', 'গই', 'অলত', '?', '১৩৪/আঁই', 'যাইয়্যুম', 'বই', '১৩৫/বইন', 'দোয়া', 'গরিও', '১৩৬/আকাশত', 'মেঘ', 'গইজ্জে', '১৩৫/আম্মা', 'পেপার', 'পরিবল্লা', 'লইয়ে', 'পঅল্লা', '১৩৭/রইদ', 'ওইট্টে', '১৩৮/', 'ইবা/ইতে', 'পন্না', 'পরের', '১৩৯/গম', 'গরি', 'টাইট', 'গরি', 'বাঁআদ', '১৪০/ইতে', 'ভালা', 'মানুষ', '১৪১/আঁই', 'গম', 'নাই', '১৪২/আইজ্জা', 'হাইসস্যা', 'দিয়্যরে', 'ইলিম', 'মাছ', 'রাইন্ধি', '১৪৩/তোঁয়ার', 'চম্মা', 'গান', 'ভাঙ্গি', 'গিয়্যই', '১৪৪/আইজ্জা', 'সারাদিন', 'ঝড়', 'অইয়্যে', '১৪৫/ইতারা', 'পুছার', 'ন', 'গরে', '?', '১৪৬/আঁই', 'নানারত', 'যাইয়্যুম', '১৪৭/আঁই', 'দাদারত', 'যাইয়্যুম', '১৪৮/আঁই', 'খালারত', 'যাইয়্যুম', '১৪৯/আঁই', 'ফুইন্যারত', 'যাইয়্যুম', '১৫০/আঁই', 'মামুরত', 'যাইয়্যুম', '১৫১/ইতারা', 'ফন্না', 'ফরের', '১৫২/আঁই', 'তোয়ারে', 'বাইজ্জায়্যুম/মারিয়ুম', '১৫৩/তোঁয়ারা', 'কেং', 'গরর', '?', '১৫৪/আঁই', 'তোঁয়ারে', 'পুছার', 'গইজ্জি', '১৫৫/তুঁই', 'ইয়া', 'কি', 'খাঁজ', 'গরর', '?', '১৫৬/আঁই', 'নেইট্টি', '১৫৭/আঁই', 'ঘুম', 'গিয়ইলাম', 'দে', '১৫৮/আঁরত্তো', 'ঠ্যাং', 'মরেদ্দে', '১৫৯/', 'আঁরত্তো', 'গা', 'মরেদ্দে', '১৬০/আঁই', 'কুলি', 'গরি', 'আঁই', '১৬১/আঁই', 'দাঁত', 'বেরাশ', 'গরি', 'আঁইর', '১৬২/আইজ্জা', 'কি', 'ছালন', 'রাইন্ধ/কি', 'মাছ', 'রাইন্ধ', '?', '১৬৩/ইতে', 'কি', 'চঅরী', 'গরেদ্দে', '?', '১৬৪/ইতে', 'পন্নাত', 'বেশী', 'গম', '১৬৫/ফুয়ানা', 'মাছ', 'দি', 'ভাত', 'খাইয়ি', '১৬৬/ইতে/ইবা', 'বেশী', 'লেডা', '১৬৭/নোয়া', 'খঅর', 'গাআদ্দি', 'বেরাইত', 'যাইয়্যুম', '১৬৮/ভাত্তাচনা', ',', 'গোআইজ্জা', 'চনা', 'চিনন', 'নে', '?', '১৬৯/চুরহাডা', 'বেক্কিন', 'গইয়্যুম', 'খাই', 'ফেলাইয়ে', '১৭০/চুরহাডা', 'বেক্কিন', 'নাকিল', 'খাই', 'ফেলাইয়ে', '১৭১/', 'আঁততু', 'উঁতাদ্দে', '১৭২/আঁই', 'নুয়াহালি', 'যাইয়্যুম', '১৭৩/আঁই', 'ভারসিটিত', 'যাইয়্যুম', '১৭৪/আঁই', 'ডাহা', 'যাইয়্যুম', '১৭৫/আঁই', 'চিটাং', 'যাইয়্যুম', '১৭৬/তোঁয়ারা', 'ঠিক', 'সময়ে', 'ঘুম', 'যাইয়্যু', 'গই', '১৭৭/অলত', 'গিয়রে', 'জিনিস', 'পত্র', 'লই', 'আইসসুম', '১৭৮/আম্মা', 'বিন্না', 'লা', 'রুটি', 'বানার', '১৭৯/সমুদ্র', 'চরত', 'যাইয়্যুম', '১৮০/জাইল্লা', 'বোট', 'লইঅরে', 'মাছ', 'ধইত্ত', 'গিইয়ে', '১৮১/মাছ', 'বেচিত', 'যাইব', '১৮২/কঅদিন', 'পর', 'বোট', 'লামাই', 'মাছ', 'ধইত্ত', 'যাইব', '১৮৩/দোআইন্যা', 'বেশী', 'মারু', '১৮৪।', 'ইতে', '(', 'ছেলে', ')', '/ইবা', '(', 'মেয়ে', ')', 'গান', 'ফুনের', '১৮৫/দোআইন্যা', 'অয়া', 'ভালা', 'আছে', '১৮৬/আঁরা', 'ওটোলত', 'ভাত', 'খাইত', 'যাইয়্যুম', '১৮৭/আঁই', 'ঘুম', 'যাইত', 'যাইর', '১৮৮/আম্মা', 'ঘরর', 'হাম', 'গরের', '১৮৯/আব্বা', 'অফিসত', 'গিয়্যে', '১৯০/আঁই', 'হতবার', 'খইয়্যি', 'ইতারে', 'ইসাব', 'নাই', '১৯১/আঁতো', 'ভুখ', 'নঅ', 'লাগে', '১৯২/খাই', 'ফেল', ',', 'শতানি', 'ন', 'গইজ্জ', '১৯৩/এং', 'কেন', ',', 'হতা', 'ন', 'ফুনে', '!', '১৯৪/ইতে', 'গম', 'পোয়া', '১৯৫/ইবা', 'গম', 'মাইয়া', 'পোয়া', '১৯৬/ইতে', 'বিদশত্তু', 'ডর', 'ডিগ্রি', 'লই', 'আইস্যে', 'দে', '১৯৭/আঁই', 'ফানি/চা', 'খাইয়্যুম', '১৯৮/আঁই', 'যহন', 'ফুছার', 'গইজ্জি', 'ইতে', 'মিছা', 'মাইত্যে', '১৯৯/ইবা', '(', 'মেয়ে', ')', '/ইতে', '(', 'ছেলে', ')', 'নাচিত', 'জানে', 'ভালা', '২০০/আঁই', 'কিছু', 'টাআর', 'গরিত', 'ন', 'ফারির']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt8aCN3R464E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829757c6-d74c-46db-f99d-774ba6e541b8"
      },
      "source": [
        "# load data\n",
        "filename = '/content/T.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "# stemming of words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:2000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeff1.আঁই', 'ন', 'গইজ্জুম-আমি', 'করব', 'না', '2.তোঁয়ারা', 'গম', 'আছনে', '?', '-', 'তোমরা', 'ভাল', 'আছ', '?', '৩।', 'অঁনে', 'গম', 'আছন্নে', '?', '-', 'আপনি', 'ভাল', 'আছেন', '?', '৪।অনরা', 'কেন', 'আছন', '?', '-', 'আপনারা', 'কেমন', 'আছেন', '?', '৫।ইতারা', 'কেং', 'গরের', '?', '-', 'তারা', 'কি', 'করছে', '?', '৬/আঁর', 'বাড়ি', 'খাসসবাজার-', 'আমার', 'বাড়ি', 'কক্সবাজার', '?', '৭/ইন', 'ইতে', 'চাইয়েদেরী-সে', 'এটা', 'চাইছে', '৮/আই', 'কেং', 'গইরত্তাম-', 'আমি', 'কি', 'করব', '৯/কুঁইইরে', 'হঅরাইইয়ে-', 'কুকুর', 'এ', 'কামড়াইছে', '১০/মুশা', 'হঅরার', 'দে', 'আঁরে-', 'আমাকে', 'মশা', 'কামড়াচ্ছে', '১১/বিলাই', 'মাছ', 'চুরি', 'গরি', 'খাই', 'ফেলাইয়ে', 'দে', '-বিড়াল', 'মাছ', 'চুরি', 'করে', 'খেয়ে', 'ফেলেছে', '১২/দিয়া', 'ফইল্লে', 'দি', 'ফেল', 'গই-যার', 'যা', 'মুল্য', 'প্রাপ্র্য', '১৩/আঁই', 'কি', 'হইতাম', 'এরে-আমি', 'কি', 'বলতাম', 'এখানে', '১৪/ইতারা', 'গম', 'মারেদ্দে-', 'তারা', 'গল্প', 'করছে।', '১৫/বিইন্না', 'ভাত', 'খাই-সকালে', 'ভাত', 'খাই', '১৬/দুইজ্জা', 'হাআলি', 'ইছা', 'মাছ', 'দিই', 'অরে', 'ভাত', 'খাইই', 'দেএরি-দুপুরবেলা', 'খালি', 'চিংড়ি', 'মাছ', 'দিয়ে', 'ভাত', 'খেয়েছি', '১৭/আছরত্তত', 'আইসসুম', 'আঁই-', 'আমি', 'বিকালে', 'আসব', '১৮/', 'তেইল্যাচুরা', 'লাই', 'কিছু', 'রাখিত', 'ন', 'পারির-', 'তেলাপোকার', 'জন্য', 'কিছু', 'রাখতে', 'পারি', 'না', '১৯/টুট', 'টুই', 'ঢাহেদ্দে', 'আ', '–', '২০/হাট্টল', 'খাইওস', 'নে', '?', '-কাঠাল', 'খেয়েছিস', '?', '২১১/ইতাত্তো', 'বাজমালর', 'দোয়ান', 'আছে-ওর', 'মুদির', 'দোকান', 'আছে', '২১/বেজ্ঞিন', 'ঘটি', 'যার', 'গই-সব', 'কিছু', 'ঘটে', 'চলেছে', '২২/', 'ইতে', 'বিয়া', 'গইজ্জে-', 'সে', 'বিয়ে', 'করেছে', '২৩/তুয়ার', 'গিন', 'তুয়ারে', 'দি', 'ফেলাই-তোমার', 'গুলা', 'তোমাকে', 'দিয়ে', 'ফেলেছি', '২৪/মধুপুক', 'অর', 'মধু', 'ইন-মৌমাছির', 'মধু', 'এগুলা', '২৫/', 'আঁই', 'খুইশুল', 'খাইইমু-', 'আমি', 'আখ', 'খাব', '২৬/আইজ্জা', 'কি', 'রাইন্ধঅ', '?', '-', 'আজকে', 'কি', 'রান্না', 'করেছ', '?', '২৭/এত', 'মাছর', 'মাঝত', 'ইবা', 'কি', 'মাছ-এত', 'মাছের', 'মাঝে', 'এটা', 'কি', 'মাছ', '?', '২৮/মাছ', 'ইবা', 'গম', 'গরি', 'ডঅর-', 'মাছটি', 'বেশ', 'বড়', '২৯/', 'কুরা', 'গোসস', 'খাইবা', 'নে', '?', '-', 'মুরগীর', 'মাংস', 'খাবে', '?', '৩০/কুরা', 'কিন্নন', 'নেকি', '?', '-মুরগী', 'কিনেছেন', '?', '৩১/কুরার', 'ছ', 'গিন', 'চো্রে', 'লই', 'গিইয়েগই-মুরগীর', 'বাচ্চা', 'চোরে', 'নিয়ে', 'গেছে', '৩২/সেমিট', 'গিন', 'হরে', 'রাইক্ক', 'তুঁই', '?', '-সিমেন্ট', 'কোথায়', 'রেখেছ', '?', '৩৩/ছাই', 'এনা', 'ছইড', 'ন', 'গরিঅ', '–দেখি', 'না', 'কি', 'হয়', 'অস্থির', 'হইও', 'না', '৩৪/আঁরার', 'বাসাত', 'মেমান', '(', 'গরবা', ')', 'আইসসে-', 'আমাদের', 'বাসায়', 'মেহমান', 'আসছে', '৩৫/আঁই', 'ন', 'ফাইজ্জুম-আমি', 'পারব', 'না', '৩৬/বাআরে', 'খাঁউয়া', 'ডাহের-বাইরে', 'কাক', 'ডাকছে', '৩৭।ইতারা', 'কি', 'গইজ্জেদে-', 'তারা', 'কি', 'করেছে', '?', '৩৮/গঅলইস', 'গিন', 'দুই', 'দিও-গ্লাস', 'গুলা', 'ধুয়ে', 'দাও', '৩৯/খাঁফ', 'পিরিচ', 'গিন', 'সুন্দর', 'আছে-', 'কাপ', 'পিরিচ', 'গুলা', 'সুন্দর', 'আছে', '৪০/হইজ্জা', 'ন', 'গইজ্জ', 'ছাই', 'এক্কা', '–', 'ঝগড়া', 'করিও', 'না', '৪১/অড়ি', 'হইজ্জা', 'ন', 'গরিস-', 'মেয়ে', 'ঝগড়া', 'করিও', 'না', '৪২/ইতে', 'বেক্কিন', 'জানে-সে', 'সব', 'জানে', '৪৩/অনে', 'বেক্কিন', 'জানন-', 'আপনি', 'সব', 'জানেন', '৪৪/এক্কানা', 'হইজ্জা', 'ন', 'গইজ্জন', 'ছাই-ঝগড়া', 'করিয়েন', 'না', '৪৫/ইবা', 'বেক্কিন', 'জানে-', 'সে', 'সব', 'জানে', '৪৬/তোয়ার', 'নাম', 'কি', '?', '-তুমার', 'নাম', 'কি', '?', '৪৭/ইতাত্তো', 'বাজমালর', 'দোয়ান', 'আছে-ওর', 'মুদির', 'দোকান', 'আছে', '৪৮/আঁই', 'খেলাইত', 'যাইর', 'এহন-আমি', 'এখন', 'খেলতে', 'যাচ্ছি', '৪৯/চগলাইট', 'খাইবা', 'নে', '?', '-চকলেট', 'খাবে', '?', '৫০/আঁই', 'জানি-', 'আমি', 'জানি', '৫১/আঁই', 'ভাত', 'খাইর', '–', 'আমি', 'ভাত', 'খাই', '৫২/হইতর', '(', 'কবুতর', ')', 'গিন', 'বাদি', 'রাখন', 'ছাই-কবুতরগুলা', 'বেধে', 'রাখেন', 'দেকি', '৫৩/অন্দুর', 'রে', 'বেক্কিন', 'নাশ', 'গইজ্জে-ইদুর', 'সব', 'নষ্ট', 'করে', 'ফেলছে', '৫৪/আঁত্তো', 'কিছু', 'গম', 'ন', 'লাগের-আমার', 'কিছু', 'ভাল', 'লাগছে', 'না', '৫৫/কি', 'দি', 'হাইঅন', '?', '–', 'কি', 'দিয়ে', 'খেয়েছেন', '?', '৫৬/ভাত', 'হাইঅইনে', 'নে', '?', '-', 'ভাত', 'খেয়েছেন', '?', '৫৭/ইবা/ইতে', 'আঁকিত', 'জানে', 'ভালা-', 'সে', 'ভাল', 'আঁকতে', 'জানে', '৫৮/হাইল্লা', 'যাইইয়ুম', 'আঁই', 'শহরত-আমি', 'কাল', 'শহরে', 'যাব', '৫৯/উগগা', 'ফুয়ারে', 'ভর', 'নে', 'ফাতল', 'তুলি', 'ন', 'ছায়-যে', 'বাবা', 'মা', 'সন্তান', 'কে', 'অনাদরে', 'মানুষ', 'করেছে', '৬০/শাট', 'ফেন', 'পরি', 'হরে', 'যঅর', '?', '-কোথায়', 'যাচ্ছ', '(', 'ছেলে/পুরুষ', ')', '৬১/মন', 'দি', 'গরিবা', '–মনযোগ', 'দিয়ে', 'কর', '৬২/মন', 'দি', 'হাআম', 'গরিও', '–মনোযোগ', 'দিয়ে', 'কাজ', 'কর', '৬৩/আঁই', 'অনরে', 'ন', 'চিনি-আমি', 'আপনাকে', 'চিনি', 'না', '৬৪/কিছু', 'মনত', 'ন', 'গইজ্জন', '–', 'কিছু', 'মনে', 'করিও', 'না', '৬৫/তুঁই', 'ঝাউরা-', 'তুমি', 'শয়তান', '৬৬/ইতে', 'মরি', 'গিইইয়েগই-সে', 'মারা', 'গেছে', '৬৭/অসুখত', 'পইজ্জে', 'দেএরি-সে', 'অসুস্থ', '৬৮/৬', 'টাত', 'শেষ', 'অইইয়ে-', '৬', 'টায়', 'শেষ', 'হয়ে', 'গেছে', '৬৯/আঁই', 'পুছার', 'গইজ্জুম-আমি', 'জিজ্ঞেস', 'করব', '৭০/আজার', 'আজার', 'মানুষ', 'অর', 'ঢল', 'নাইম্মে-হাজার', 'হাজার', 'মানুষের', 'ঢল', 'নেমেছে', '৭১/কেং', 'গইজ্জ', 'দে', '–', 'কি', 'করেছে', '?', '৭২/ডর', 'পুয়া', ',', 'মাইজ্জা', 'পোয়া', ',', 'গুরা', 'পোয়া', 'মিলি', 'কেন', 'গরর-বড়', 'মেয়ে', ',', 'মেজ', 'মেয়ে', 'আর', 'ছোট', 'মেয়ে', 'মিলে', 'কি', 'করছ', '?', '৭৩/ইতার', 'মাইজ্জা', 'যেউরফা', 'আ', 'গম', '–', 'তার', 'মেঝ', 'মেয়েটা', 'ভাল', '৭৪/আঁই', 'ইতারে', 'গম', 'ফাই-আমি', 'তাকে', 'পছন্দ', 'করি', '৭৫/আতিক্কা', 'মিক্কান', 'ডরাইয়ে', 'দে-হঠাং', 'করে', 'ভয়', 'পেয়েছি', '৭৬/আঁরা', 'ন', 'যাইইউম-আমরা', 'যাব', 'না', '৭৭/এক্কু', 'পোঁয়াতি', 'যাইয়ুম-', 'একসাথে', 'যাব', '৭৮৮/আঁই', 'কেং', 'গইত্তাম-আমি', 'কি', 'করব', '৭৮/অনরে', 'পোছার', 'গইজ্জিদে', 'ননে-আপনাকে', 'জিজ্ঞেস', 'করেছি', '৭৯/আঁর', 'অসুখ', 'লাগের-', 'আমার', 'অসুস্থ', 'লাগছে', '৮০/চুখত', 'ভুল', 'দেহির-আমি', 'চোখে', 'ঠাহর', 'করতে', 'পারছি', 'না', '৮১/চইল', 'ফুরা', 'ই', 'গিইওই-চাল', 'শেষ', 'হয়ে', 'গেছে', '৮২/বারাবারি', 'ন', 'গরিঅ', '–বাড়াবাড়ি', 'করিও', 'না', '৮৩/বাআন্ডাত', 'বইঅ', 'বাতাস', 'আছে', 'এরে-বারান্দায়', 'বস', ',', 'বাতাস', 'আছে', 'এখানে', '৮৪/আতিনার', 'দুয়ারত', 'বইসসে', 'মেমান', '(', 'গরবা', ')', '-সামনের', 'ঘরে', 'বসছে', 'মেহমান', '৮৫/দইনত', 'গরে', 'থাকে', 'দে', 'আব্বা-দক্ষিন', 'এর', 'ঘরে', 'থাকে', 'আব্বা', '৮৬/উডান', 'নান', 'এক্কা', 'ঝাড়ু', 'দ', 'গই-উঠান', 'তা', 'একটু', 'ঝাড়ু', 'দাও', 'ত', '৮৭/ফুরইন', 'হরে', 'রাইক্কে', 'মম্মা', '(', 'আম্মা', ')', '?', '-আম্মা', 'ঝাড়ু', 'কোথায়', 'রাখছে', '?', '৮৮/অউলাত', 'রাধিরদে-রান্নাঘরে', 'রান্না', 'করছি', '৮৯/অউলাত', 'মম্মা', '(', 'আম্মা', ')', 'গোসস', 'রাঁধের-রান্না', 'ঘরে', 'আম্মা', 'মাংস', 'রান্না', 'করছে', '৯০/ন', 'লড়িও-নড়িও', 'না', '৯১/অডি/অড়ি', 'ডর', 'লাগেদ্দে', '–', 'বোন', 'ভয়', 'ভয়', 'লাগছে', '৯২/এক্কানা', 'আচ্ছে', 'ছালাছেনা/ছালাছুনা-একটু', 'আস্থে', 'চালাও', '৯৩/এক্কানাইচ্ছ', 'পো্যাইন', 'এদিন', 'গা', 'শতানিগরে', 'আ-', 'পিচ্ছি', 'বাচ্চা', 'টা', 'খুব', 'দুষ্টু', '৯৪/গা', 'ধুই', 'আঁই', ',', 'তয়', 'হতা', 'খুইয়ুম-গোসল', 'করে', 'আসি', ',', 'কথা', 'বলব', '৯৫/কি', 'খইঅ', 'দে', '?', '-কি', 'খাচ্ছ', '?', '৯৬/আঁর', 'তু', 'টিয়া', 'নাই-', 'আমার', 'টাকা', 'নাই', '৯৭/', 'তুঁই', 'কি', 'হঅদ্দে', '?', '-', 'তুমি', 'কি', 'বলছ', '?', '৯৮/তুঁই', 'কি', 'হইবা', 'খঅ-তুমি', 'কি', 'বলছ', 'বল', '?', '৯৯/তুঁই', 'কি', 'গরর', 'দে', '?', '-তুমি', 'কি', 'করছ', '?', '১০০/তুই', 'গম', 'আছ', 'নে', '?', '-তুমি', 'ভাল', 'আছ', '?', '১০১/আঁই', 'যাই', 'ন', 'ফারিদ্দে-আমি', 'যেতে', 'পারছি', 'না', '১০২/এক্কিনিচ্চ', 'পুজলি', 'এংকেন', 'লাফাই', 'ফারে', '!', '-একটুখানি', 'বাচ্চা', 'এত', 'লাফায়', '!', '১০৩/এবার', 'ন', 'অইব-এবার', 'হবে', 'না', '১০৪/আঁরা', 'ফুয়াতি', 'বই', 'থাইক্কি-', 'আমি', 'একসাথে', 'বসে', 'আছি', '১০৫/আঁরা', 'ফুয়াতি', 'খাম', 'গরি', '–', 'আমরা', 'এক', 'সাথে', 'কাজ', 'করি', '১০৬/তুই', 'এন', 'গরর', 'ক্যা-তুমি', 'এমন', 'করছ', 'ক্যান', '?', '১০৭/', 'কঅত্তে', 'যাইবান', '?', '-', 'কখন', 'যাবেন', '?', '১০৮/হ', 'টিয়া', 'লইবান', '?', '-কত', 'টাকা', 'নিবেন', '?', '১০১/অনে', 'বেশী', 'চঅন', 'দেয়া', '!', '!', '!', '!', '-আপনি', 'বেশী', 'চাচ্ছেন', '১০২/আরেক্কানা', 'হঅমন-আরেকটু', 'কমান', '১০৩/বাআজি', 'আঁরা', 'ইনর', 'দাম', 'জানি-আরে', 'বাবা', 'আমি', 'এসবের', 'দাম', 'জানি', '১০৪/এন্ডিলা', 'কা', 'দাম', 'চাইত', 'লাইজ্ঞন-', 'এরকম', 'দাম', 'চাচ্ছেন', 'কেন', '?', '১০৯/তুঁই', 'এন', 'গরর', 'কা', '?', '-তুঁই', 'এরকম', 'করছিস', 'কেন', '?', '১১০/আঁই', 'ন', 'জানি-আমি', 'জানি', 'না', '১১১/ইতারা', 'খইজ্জা', 'গরের-তারা', 'ঝগড়া', 'করছে', '১১২/তুই', 'আঁর', 'জাউরা', 'ভাই/বইন', '–', 'তুই', 'আমার', 'দুস্টু/শয়তান', 'বোন/ভাই', '১০৯।তুই', 'আঁর', 'জাউরা', 'বইন', '১১০/ইবা', 'রাইত্যা', 'কুডুর', 'কুডুর', 'গরি', 'বিস্কুট', 'খাআদ্দে', '১১৩/ইতে', 'কি', 'গরেদ্দে', '?', '-সে', 'কি', 'করছে', '?', '১১৪/মাইয়া', 'পোয়া', 'ইবা', 'বেশী', 'টাউট-মেয়েটা', 'বেশী', 'দুইনম্বর', '১১৫/মরতপোয়া', 'ইতে', 'বেশী', 'টাউট', '–ছেলেটা', 'বেশী', 'দুই', 'নম্বর', '১১৬/আঁই', 'তো্যারে', 'গম', 'ফাই-', 'আমি', 'তোমাকে', 'পছন্দ', 'করি', '১১৭/তুঁই', 'আঁরে', 'গম', 'ফাই', 'অ', 'দেএরী-তুমি', 'আমাকে', 'পছন্দ', 'কর', '১১৮/মরার', 'ধইল', 'ফরি', 'কা', 'রইঅস', '?', '-এরকম', 'মুখ', 'ভার', 'করে', 'আছ', 'কেন', '?', '১১৯/ঘাট', 'ফান', 'বাধি', 'অরে', 'নাম-শক্ত', 'করে', 'কাজে', 'নেমে', 'পড়', '১১৮/', 'ঘাট', 'ফান', 'বাধিওরে', 'নাইম্মে', 'ইতে', '১২০/', 'মাইয়া', 'অউর', 'বাড়িত', 'গিয়ে', '-মেয়ে', 'শশুরবাড়ি', 'গেছে', '১২১/অড়া', 'বেশি', 'খতা', 'খস', 'তুঁই-', 'ছেলে', 'বেশি', 'কথা', 'বলিস', '১২২/আঁই', 'তর', 'মশকইজ্জা', 'লাগি', 'নেকি', '?', '-আমার', 'সাথে', 'তোমার', 'ঠাট্টার', 'সম্পর্ক', '?', '১২৩/মুরব্বির', 'লগে', 'শতানি', 'গরস', 'দে', 'নেকি-মুরব্বিদের', 'সাথে', 'কেন', 'ফাইজলামি', 'কর', '?', '১২৪/মশকারা', 'গরে', 'ইতে', 'বেশী-সে', 'বেশী', 'মজা/ফাইজালামি', 'করে', '১২৫/অ', 'বাজি', 'তল্লা', 'পেট', 'পুরেদ্দে-', 'তোর', 'কথা', 'মনে', 'পড়ছে', '১২৬/বাড়িত', 'আইওন', ',', 'বেরাইওত-', 'বাসায়', 'বেড়াতে', 'আসেন', '১২৭/খাম', 'শেস', 'গরি', 'আইসসুম-', 'কাজ', 'শেষ', 'করে', 'আসব', '১২৮/বেক্কিন', 'সেট', 'গরির-সব', 'ঠিক', 'করছি', '১২৯/আঁর', 'লাইও', 'দোয়া', 'গইজ্জন-', 'আমার', 'জন্যও', 'দোয়া', 'করিয়েন', '১৩০/কি', 'গরন', 'দে', '?', '-কি', 'করছেন', '?', '১৩১/কি', 'হবর', '?', '-কি', 'খবর', '?', '১৩২/হত্যে', 'যাইবান', 'গই', 'অলত', '?', '-কখন', 'চলে', 'যাবেন', 'হলে', '?', '১৩৪/আঁই', 'যাইয়্যুম', 'বই-আমি', 'চলে', 'যাব', '১৩৫/বইন', 'দোয়া', 'গরিও-বোন', 'দোয়া', 'করিও', '১৩৬/আকাশত', 'মেঘ', 'গইজ্জে-আকাশে', 'মেঘ', 'করেছে', '১৩৫/আম্মা', 'পেপার', 'পরিবল্লা', 'লইয়ে', 'পঅল্লা-আম্মু', 'মনে', 'হয়', 'পেপারটা', 'নিছে', 'পড়ার', 'জন্য', '১৩৭/রইদ', 'ওইট্টে-রোঁদ', 'উঠছে', '১৩৮/', 'ইবা/ইতে', 'পন্না', 'পরের-সে', 'পড়াশুনা', 'করছে', '১৩৯/গম', 'গরি', 'টাইট', 'গরি', 'বাঁআদ-ভাল', 'করে', 'টাইট', 'করে', 'বাঁধ', '১৪০/ইতে', 'ভালা', 'মানুষ-সে', 'ভাল', 'মানুষ', '১৪১/আঁই', 'গম', 'নাই-আমি', 'ভাল', 'নাই', '১৪২/আইজ্জা', 'হাইসস্যা', 'দিয়্যরে', 'ইলিম', 'মাছ', 'রাইন্ধি-আজ', 'বিচি', 'দিয়ে', 'ইলিশ', 'মাছ', 'রান্না', 'করেছি', '১৪৩/তোঁয়ার', 'চম্মা', 'গান', 'ভাঙ্গি', 'গিয়্যই-আমার', 'চশমা', 'ভেঙে', 'গেছে', '১৪৪/আইজ্জা', 'সারাদিন', 'ঝড়', 'অইয়্যে-আজ', 'সারাদিন', 'বৃষ্টি', 'হয়েছে', '১৪৫/ইতারা', 'পুছার', 'ন', 'গরে', '?', '-তারা', 'জিজ্ঞেস', 'করে', 'নি', '?', '১৪৬/আঁই', 'নানারত', 'যাইয়্যুম-আমি', 'নানা', 'বাড়ি', 'যাব', '১৪৭/আঁই', 'দাদারত', 'যাইয়্যুম-আমি', 'দাদা', 'বাড়ি', 'যাব', '১৪৮/আঁই', 'খালারত', 'যাইয়্যুম-আমি', 'খালার', 'বাড়ি', 'যাব', '১৪৯/আঁই', 'ফুইন্যারত', 'যাইয়্যুম-আমি', 'ফুফুর', 'বাড়ি', 'যাব', '১৫০/আঁই', 'মামুরত', 'যাইয়্যুম-আমি', 'মামার', 'বাড়ি', 'যাব', '১৫১/ইতারা', 'ফন্না', 'ফরের-তারা', 'পড়াশুনা', 'করছে', '১৫২/আঁই', 'তোয়ারে', 'বাইজ্জায়্যুম/মারিয়ুম-আমি', 'তোমাকে', 'মারব', '১৫৩/তোঁয়ারা', 'কেং', 'গরর', '?', '-তোমরা', 'কি', 'করছ', '?', '১৫৪/আঁই', 'তোঁয়ারে', 'পুছার', 'গইজ্জি-আমি', 'তোমাকে', 'জিজ্ঞেস', 'করেছি', '১৫৫/তুঁই', 'ইয়া', 'কি', 'খাঁজ', 'গরর', '?', '-তুমি', 'এখন', 'কিকাজ', 'করছ', '?', '১৫৬/আঁই', 'নেইট্টি-আমি', 'শুয়ে', 'আছি', '১৫৭/আঁই', 'ঘুম', 'গিয়ইলাম', 'দে-আমি', 'ঘুমিয়ে', 'গেছিলাম', '১৫৮/আঁরত্তো', 'ঠ্যাং', 'মরেদ্দে-আমার', 'পা', 'ব্যাথা', 'করছে', '১৫৯/', 'আঁরত্তো', 'গা', 'মরেদ্দে-আমার', 'শরীর', 'ব্যাথা', 'করছে', '১৬০/আঁই', 'কুলি', 'গরি', 'আঁই-আমি', 'কুলি', 'করে', 'আসছি', '১৬১/আঁই', 'দাঁত', 'বেরাশ', 'গরি', 'আঁইর-আমি', 'দাঁত', 'ব্রাশ', 'করে', 'আসছি', '১৬২/আইজ্জা', 'কি', 'ছালন', 'রাইন্ধ/কি', 'মাছ', 'রাইন্ধ-আজ', 'কি', 'তরকারি', 'রান্না', 'করেছ', '?', '১৬৩/ইতে', 'কি', 'চঅরী', 'গরেদ্দে', '?', '-সে', 'কোথায়', 'চাকরী', 'করছে', '?', '১৬৪/ইতে', 'পন্নাত', 'বেশী', 'গম-সে', 'পড়ালেখায়', 'বেশ', 'ভাল', '১৬৫/ফুয়ানা', 'মাছ', 'দি', 'ভাত', 'খাইয়ি-আমি', 'শুটকি', 'দিয়ে', 'ভাত', 'খেয়েছি', '১৬৬/ইতে/ইবা', 'বেশী', 'লেডা-সে', 'খুব', 'চিকন', '১৬৭/নোয়া', 'খঅর', 'গাআদ্দি', 'বেরাইত', 'যাইয়্যুম-আমি', 'নতুন', 'কাপড়', 'পরে', 'বেড়াতে', 'যাব', '১৬৮/ভাত্তাচনা', ',', 'গোআইজ্জা', 'চনা', 'চিনন', 'নে', '?', '-ভাত', 'শালিক', ',', 'গোশালিক', 'চিনেন', '?', '১৬৯/চুরহাডা', 'বেক্কিন', 'গইয়্যুম', 'খাই', 'ফেলাইয়ে-কাঠবিড়ালী', 'সব', 'পেয়ারা', 'খেয়ে', 'ফেলছে', '১৭০/চুরহাডা', 'বেক্কিন', 'নাকিল', 'খাই', 'ফেলাইয়ে-কাঠবেড়ালী', 'সব', 'নারকেল', 'খেয়ে', 'ফেলছে', '১৭১/', 'আঁততু', 'উঁতাদ্দে-আমার', 'চুলকাচ্ছে', '১৭২/আঁই', 'নুয়াহালি', 'যাইয়্যুম-আমি', 'নোয়াখালী', 'যাব', '১৭৩/আঁই', 'ভারসিটিত', 'যাইয়্যুম-আমি', 'ভার্সিটি', 'যাব', '১৭৪/আঁই', 'ডাহা', 'যাইয়্যুম-আমি', 'ঢাকা', 'যাব', '১৭৫/আঁই', 'চিটাং', 'যাইয়্যুম-আমি', 'চিটাগং', 'যাব', '১৭৬/তোঁয়ারা', 'ঠিক', 'সময়ে', 'ঘুম', 'যাইয়্যু', 'গই-তোমরা', 'ঠিক', 'সময়ে', 'ঘুমিয়ে', 'যেও', '১৭৭/অলত', 'গিয়রে', 'জিনিস', 'পত্র', 'লই', 'আইসসুম-আমি', 'হলে', 'গিয়ে', 'জিনিস', 'পত্র', 'নিয়ে', 'আসব', '১৭৮/আম্মা', 'বিন্না', 'লা', 'রুটি', 'বানার-আম্মা', 'সকালের', 'জন্য', 'রুটি', 'বানাচ্ছে', '১৭৯/সমুদ্র', 'চরত', 'যাইয়্যুম-আমি', 'সমুদ্র', 'দেখতে', 'যাব', '১৮০/জাইল্লা', 'বোট', 'লইঅরে', 'মাছ', 'ধইত্ত', 'গিইয়ে-জেলে', 'বোট', 'নিয়ে', 'মাছ', 'ধরতে', 'গেছে', '১৮১/মাছ', 'বেচিত', 'যাইব-মাছ', 'বেচতে', 'যাব', '১৮২/কঅদিন', 'পর', 'বোট', 'লামাই', 'মাছ', 'ধইত্ত', 'যাইব-কয়দিন', 'পর', 'বোট', 'নিয়ে', 'মাছ', 'ধরতে', 'যাবে', '১৮৩/দোআইন্যা', 'বেশী', 'মারু-দোকানদারটা', 'দুই', 'নাম্বার', '১৮৪।', 'ইতে', '(', 'ছেলে', ')', '/ইবা', '(', 'মেয়ে', ')', 'গান', 'ফুনের', '–সে', 'গান', 'শুনছে', '১৮৫/দোআইন্যা', 'অয়া', 'ভালা', 'আছে-দোকানদার', 'টা', 'ভাল', 'আছে', '১৮৬/আঁরা', 'ওটোলত', 'ভাত', 'খাইত', 'যাইয়্যুম-আমরা', 'হোটেলে', 'ভাত', 'খেতে', 'যাব', '১৮৭/আঁই', 'ঘুম', 'যাইত', 'যাইর-আমি', 'ঘুমাতে', 'যাচ্ছি', '১৮৮/আম্মা', 'ঘরর', 'হাম', 'গরের-আম্মা', 'ঘরের', 'কাজ', 'করছে', '১৮৯/আব্বা', 'অফিসত', 'গিয়্যে-আব্বা', 'আফিসে', 'গেছেন', '১৯০/আঁই', 'হতবার', 'খইয়্যি', 'ইতারে', 'ইসাব', 'নাই-আমি', 'কতবার', 'যে', 'তাকে', 'বলেছি', 'হিসাব', 'নাই', '১৯১/আঁতো', 'ভুখ', 'নঅ', 'লাগে-আমার', 'ক্ষুধা', 'লাগে', 'নি', '১৯২/খাই', 'ফেল', ',', 'শতানি', 'ন', 'গইজ্জ-খেয়ে', 'ফেল', ',', 'দুস্টামী', 'করিও', 'না', '১৯৩/এং', 'কেন', ',', 'হতা', 'ন', 'ফুনে', '!', '!', '-এমন', 'কেন', 'কথা', 'শুনে', 'না', '!', '!', '১৯৪/ইতে', 'গম', 'পোয়া-সে', 'ভাল', 'ছেলে', '১৯৫/ইবা', 'গম', 'মাইয়া', 'পোয়া-সে', 'ভাল', 'মেয়ে', '১৯৬/ইতে', 'বিদশত্তু', 'ডর', 'ডিগ্রি', 'লই', 'আইস্যে', 'দে-সে', 'বিদেশ', 'থেকে', 'ডিগ্রি', 'নিয়ে', 'আসছে', '১৯৭/আঁই', 'ফানি/চা', 'খাইয়্যুম-আমি', 'চা/পানি', 'খাব', '১৯৮/আঁই', 'যহন', 'ফুছার', 'গইজ্জি', 'ইতে', 'মিছা', 'মাইত্যে-', 'আমি', 'যখন', 'জিজ্ঞেস', 'করেছি', 'তখন', 'সে', 'মিথ্যা', 'কথা', 'বলেছে', '১৯৯/ইবা', '(', 'মেয়ে', ')', '/ইতে', '(', 'ছেলে', ')', 'নাচিত', 'জানে', 'ভালা', '–সে', 'ভাল', 'নাচতে', 'জানে', '২০০/আঁই', 'কিছু', 'টাআর', 'গরিত', 'ন', 'ফারির-আমি', 'কিছু', 'বুঝতে', 'পারছি', 'না', '|', '২০১/', 'আলুর', 'ছান্নি', 'আর', 'ফাইন্না', 'ডাল', 'দিইঅরে', 'ভাত', 'খাইয়্যি-আলু', 'ভর্তা', 'আর', 'পানি', 'পানি', 'ডাল', 'দিয়ে', 'ভাত', 'খাইছি।', '২০২/', 'চুদুর', 'বুদুর', 'ন', 'গইজ্জ-', 'উল্টা', 'পাল্টা', 'করিও', 'না।', '২০৩/', 'এত্তর', 'অয়িদে', 'আলু', 'খাই', 'খাই-আলু', 'খাই', 'খাই', 'এত', 'বড়', 'হইছি।', '২০৪/', 'ইতারে', 'ডাক-তাকে', 'ডাক।', '২০৫/', 'ইতারে', 'ডাকছেনা-', 'তাকে', 'ডাক।', '২০৬/আই', 'ন', 'বইস্যুম-আমি', 'বসব', 'না', '২০৭/', 'কুইজ্জা', 'ত', 'পাশে', 'ছ', 'গই-খড়ের', 'গাদার', 'পাশে', 'দেখ।', '২০৮/', 'ইতার', 'খতা', 'ইতারে', 'কা', 'লাগর-এর', 'কথা', 'একে', 'কেন', 'লাগাচ্ছ।', '২০৯/', 'তুয়ারা', 'মাস্টাররে', 'সম্মান', 'গরিবা-তুমরা', 'মাস্টাররে', 'সম্মান', 'করবে।', '২১০/', 'আবিজাবি', 'খাম', 'গরিয়রে', 'লাভ', 'নাই-উল্টা', 'পালটা', 'কাজ', 'করে', 'লাভ', 'নাই।', '২১১/', 'মাছ', 'ধরিবল্লা', 'জাইল্লা', 'দইজ্জাত', 'গিয়ে-মাছ', 'ধরার', 'জন্য', 'জেলে', 'সাগরে', 'গেছে।', '২১২/এরে', 'বয়', ',', 'বইয়ার', 'আছে-এখানে', 'বস', ',', 'বাতাস', 'আছে।', '২১৩/খদ্দাত', 'চালন', 'বাড়ি', 'রাহ-পেয়ালাতে', 'তরকারী', 'বেড়ে', 'রাখ।', '২১৪/খদ্দাত', 'চালন', 'বাড়ি', 'রাইক্ক-পেয়ালাতে', 'তরকারী', 'বেড়ে', 'রাখিও।', '২১৫/আমত্য', 'আইসুসম', 'বেরাইত-আবার', 'আসব', 'বেড়াতে।', '২১৬/', 'এরে', 'এন্ডিলা', 'ছাতা', 'ক্যা-এখানে', 'এরকম', 'ময়লা', 'কেন', '?', '২১৭/ইতে', 'কেং', 'গরি', 'আরার', 'ফুয়াতি', 'এন্ডিলা', 'গরিল-সে', 'কেমন', 'করে', 'আমাদের', 'সাথে', 'এমন', 'করল।', '২১৮/আই', 'ন', 'বুঝি-আমি', 'বুঝি', 'নাই।', '২১৯/', 'ইতে', 'ন', 'বুঝে-সে', 'বুঝে', 'না।', '২২০/ইবা', 'ন', 'বুঝে-সে', 'বুঝে', 'না।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjB0Vz0q5BsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a609246c-23c8-454b-c2cf-43713b129f7d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djgA9hMJKj83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "be91920c-b8d4-4681-f3c0-88d17fffcf8c"
      },
      "source": [
        "# load data\n",
        "filename = '/content/ct.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "# filter out stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:2000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ন', 'গম', 'গম', 'আছন', 'গই', 'গম', 'ন', 'আ', 'গই', 'অর', 'ইন', 'গম', 'ডঅর', 'ছ', 'লই', 'ছইড', 'ন', 'ন', 'ন', 'ন', 'ন', 'এহন', 'গম', 'ন', 'শহরত', 'ভর', 'ন', 'যঅর', 'ন', 'মনত', 'ন', 'অর', 'ঢল', 'গরর', 'গম', 'ন', 'ন', 'বইঅ', 'দ', 'গই', 'ডর', 'আ', 'তয়', 'খইঅ', 'খঅ', 'গরর', 'গম', 'আছ', 'ন', 'ন', 'অইব', 'বই', 'এন', 'গরর', 'চঅন', 'হঅমন', 'ইনর', 'এন', 'গরর', 'ন', 'বইন', 'গম', 'গম', 'অ', 'ধইল', 'রইঅস', 'অউর', 'খস', 'তর', 'গরস', 'আইওন', 'গরন', 'হবর', 'গই', 'অলত', 'বই', 'গম', 'ঝড়', 'ন', 'গরর', 'গরর', 'গম', 'খঅর', 'গই', 'লই', 'চরত', 'পর', 'ঘরর', 'নঅ', 'ন', 'ন', 'গম', 'গম', 'ডর', 'লই', 'যহন', 'ন']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFbcmcpB5Pso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d411a47b-b255-49cd-fbf0-6215e7608bc9"
      },
      "source": [
        "# load data\n",
        "filename = '/content/T.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "# filter out stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:2000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ন', 'করব', 'গম', 'আছ', 'গম', 'আছন', 'করব', 'এ', 'গম', 'আসব', 'ন', 'আ', 'গইসব', 'অর', 'আখ', 'গম', 'ডঅর', 'বড়', 'ছ', 'লই', 'ছইড', 'ন', 'হয়', 'হইও', 'ন', 'ন', 'ন', 'সব', 'সব', 'ন', 'সব', 'এখন', 'সব', 'গম', 'ন', 'ভর', 'ন', 'যঅর', 'কর', 'কর', 'ন', 'মনত', 'ন', 'করব', 'অর', 'ঢল', 'ঢল', 'গররবড়', 'আর', 'করছ', 'আ', 'গম', 'গম', 'ভয়', 'ন', 'করব', 'ই', 'ন', 'বইঅ', 'বস', 'এর', 'দ', 'ত', 'ডর', 'ভয়', 'ভয়', 'আ', 'তয়', 'বলব', 'খইঅ', 'বলছ', 'বলছ', 'বল', 'গরর', 'করছ', 'গম', 'আছ', 'আছ', 'ন', 'এত', 'ন', 'বই', 'এক', 'এন', 'গরর', 'এমন', 'করছ', 'কখন', 'কত', 'চঅন', 'ইনর', 'এরকম', 'এন', 'গরর', 'এরকম', 'ন', 'বইন', 'গম', 'গম', 'অ', 'কর', 'ধইল', 'রইঅস', 'এরকম', 'আছ', 'পড়', 'অউর', 'খস', 'তর', 'গরস', 'কর', 'আইওন', 'আসব', 'গরন', 'হবর', 'খবর', 'গই', 'অলত', 'কখন', 'হয়', 'গম', 'ঝড়', 'ন', 'গরর', 'করছ', 'গরর', 'এখন', 'করছ', 'খঅর', 'সব', 'সব', 'লই', 'আসব', 'চরত', 'পর', 'পর', 'ঘরর', 'নঅ', 'ন', 'ন', 'এমন', 'গম', 'গম', 'ডর', 'লই', 'যহন', 'যখন', 'তখন', 'ন', 'আর', 'আর', 'ন', 'এত', 'বড়', 'ন', 'বসব', 'ত', 'ছ', 'বয়', 'বস', 'আসব', 'এরকম', 'এমন', 'ন', 'ন', 'ন']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj8fY36A5UPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b19311-7660-4af9-e539-011d0ee99d9e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "556IniPnJHG6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "2959611d-6ea1-481d-8105-ada3666641e3"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
        "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
        "tokenized_text=sent_tokenize(text)\n",
        "print(tokenized_text)\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)\n",
        "fdist = FreqDist(tokenized_word)\n",
        "print(fdist)\n",
        "fdist.most_common(2)\n",
        "\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n",
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
            "<FreqDist with 25 samples and 30 outcomes>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAE5CAYAAACK1bf2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcVb3u8e+bOZ0QEkZbEoiIE6Ig3UzCEVDh4IgiDoiIejCOR46IF0VRcThXj9NVUQEFEUWcgCMJyKAyg0AnMg+CoBJEEMgEnamT3/1j7equVGpO7equ7vfzPPV0195r1V7Vw/7VmhURmJmZlRo33AUwM7ORyQHCzMzKcoAwM7OyHCDMzKwsBwgzMytrwnAXoJW22mqrmDt3blN5V65cydSpU3NLP5LzuFwul8s1cvK0q1wFCxcufDwiti57MiJGzaOnpyea1dfXl2v6kZzH5XK58szjco3MchUAfVHhnuomJjMzK8sBwszMynKAMDOzshwgzMysLAcIMzMrK7cAIWmKpJsk3SrpTkknl0kzWdIvJN0v6UZJc4vOfTI7fq+kf8+rnGZmVl6eNYjVwMsjYldgN+AQSXuXpPkPYElE7AR8E/gKgKSdgbcBLwQOAb4naXyOZTUzsxK5BYhsiO1T2dOJ2aN0bfFDgR9n3/8aeIUkZcd/HhGrI+JB4H5gzzzKeec/lvGOH97IqQuX5fHyZmYdS5HjfhDZp/6FwE7AdyPihJLzdwCHRMTi7PlfgL2AzwF/jIifZsfPAH4bEb8uc415wDyA7u7unvnz5zdUxr8uXcvHLn+C7aaP49uv2qbufP39/XR1dTV0rZGax+VyuVyukZOnXeUq6O3tXRgRvWVPVppB18oHMBO4Atil5PgdwOyi538BtgJOAd5RdPwM4PBa12lmJvXylWtihxMWxHNPvCjWr19fd76ROguzmTwul8uVZx6Xa2SWq4DhnkkdEUuzAHFIyamHgTkAkiYAmwNPFB/PzM6OtdxmUyYys2siq9cFjz+1Jo9LmJl1pDxHMW0taWb2/VTgIOCekmQXAkdn3x8O/CGLaBcCb8tGOT0LeA5wU15lnT0rLXL10JL+vC5hZtZx8qxBdANXSLoNuBm4PCIWSPq8pNdnac4AtpR0P3Ac8AmAiLgT+CVwF3AJ8KGIWJdXQefMSm13Dz3pAGFmVpDbct8RcRvwkjLHP1P0/SrgzRXyfwn4Ul7lKzZnixQgFi9Z2Y7LmZl1BM+kBuZkTUyL3cRkZjbIAQKYvUWhick1CDOzAgcIivogXIMwMxvkAMHQKKZ/LF3JuvX5TRw0M+skDhDAlInjmTllHGvXBf9cvmq4i2NmNiI4QGS2nZbWAlzsoa5mZoADxKBtsgDxkIe6mpkBDhCDBgOEaxBmZoADxKChGoQDhJkZOEAMGuqDcBOTmRk4QAzauss1CDOzYg4Qma26xjNO8M/lq1g9kNu6gGZmHcMBIjNhnOjefCoR8MhSz4UwM3OAKDJnC+8LYWZW4ABRZGhfCHdUm5k5QBQp7AvhGoSZWY4bBkmaA5wNbAsEcHpEfKskzceBI4vK8gJg64h4UtJfgRXAOmAgInrzKmvBYBOTJ8uZmeUXIIAB4GMRsUjSZsBCSZdHxF2FBBHxVeCrAJJeB3w0Ip4seo0DI+LxHMu4gdmDy367icnMLLcmpoh4JCIWZd+vAO4GtquS5Qjg3LzKU49CH4QX7DMzA0Xkv/+BpLnA1cAuEbG8zPkuYDGwU6EGIelBYAmpeeq0iDi9wmvPA+YBdHd398yfP7+pMvb39zNl6lTefv6jrF0P57xxG6ZMqBw/+/v76erqavgaIzGPy+VyuVwjJ0+7ylXQ29u7sGITfkTk+gCmAwuBw6qkeSswv+TYdtnXbYBbgZfVulZPT080q6+vLyIiDvzqFbHDCQvi3n8uryt9M9cYaXlcLpcrzzwu18gsVwHQFxXuqbmOYpI0ETgPOCcizq+S9G2UNC9FxMPZ18eAC4A98ypnsaH9qd3MZGZjW24BQpKAM4C7I+IbVdJtDuwP/Kbo2LSsYxtJ04CDgTvyKmuxObM8ksnMDPIdxbQvcBRwu6RbsmMnAtsDRMSp2bE3ApdFxNNFebcFLkgxhgnAzyLikhzLOsgjmczMktwCRERcC6iOdGcBZ5UcewDYNZeC1eC5EGZmiWdSl5jjGoSZGeAAsZHCchuLvdyGmY1xDhAlZnVNZNqk8axYNcCy/rXDXRwzs2HjAFFCkhftMzPDAaKswZFM7qg2szHMAaKM2bO8cZCZmQNEGYNNTN44yMzGMAeIMua4BmFm5gBRztBQV9cgzGzscoAoo3guRLRhOXQzs5HIAaKM6ZMnMKtrIqvWrudfT60e7uKYmQ0LB4gKhoa6upnJzMYmB4gKCov2eckNMxurHCAqmOPJcmY2xjlAVDDbcyHMbIxzgKigMBdi8VLXIMxsbMpzy9E5kq6QdJekOyUdWybNAZKWSbole3ym6Nwhku6VdL+kT+RVzko8m9rMxro8txwdAD4WEYuy/aUXSro8Iu4qSXdNRLy2+ICk8cB3gYOAxcDNki4skzc3281MNYh/LF3JuvXB+HE1N8czMxtVcqtBRMQjEbEo+34FcDewXZ3Z9wTuj4gHImIN8HPg0HxKWt6UiePZZrPJDKwPHlnmWoSZjT1qx0xhSXOBq4FdImJ50fEDgPNItYR/AMdHxJ2SDgcOiYhjsnRHAXtFxIfLvPY8YB5Ad3d3z/z585sqY39/P11dXRscO/EPT3DvE2s5ef9Z7LLN5Jrpm7nGSMjjcrlcLtfIydOuchX09vYujIjesicjItcHMB1YCBxW5twMYHr2/auB+7LvDwd+WJTuKOCUWtfq6emJZvX19W107NhzF8UOJyyIX9z897rSN3ONkZDH5XK58szjco3MchUAfVHhnprrKCZJE0k1hHMi4vwywWl5RDyVfX8xMFHSVsDDwJyipLOzY201uCaT50KY2RiU5ygmAWcAd0fENyqkeUaWDkl7ZuV5ArgZeI6kZ0maBLwNuDCvslZSmCznVV3NbCzKcxTTvqSmodsl3ZIdOxHYHiAiTiU1JX1A0gCwEnhbVuUZkPRh4FJgPHBmRNyZY1nLmr2F94Uws7ErtwAREdcCVceGRsQpwCkVzl0MXJxD0eo2xwv2mdkY5pnUVXRvPoXx48SjK1axemDdcBfHzKytHCCqmDB+HN2bTyECHnY/hJmNMQ4QNQw2MzlAmNkY4wBRg/eFMLOxygGiBndUm9lY5QBRw+Cqrq5BmNkY4wBRw+zCvhCeTW1mY4wDRA1DNQg3MZnZ2OIAUcPW0yczacI4nnx6DU+vHhju4piZtY0DRA3jxmmwmcn9EGY2ljhA1GFw0T6PZDKzMcQBog5zvGifmY1BDhB1mO25EGY2BjlA1GFouQ3XIMxs7HCAqMNgE5PnQpjZGJLnjnJzJF0h6S5Jd0o6tkyaIyXdJul2SddL2rXo3F+z47dI6surnPUo3lku7WdkZjb65bmj3ADwsYhYJGkzYKGkyyPirqI0DwL7R8QSSa8CTgf2Kjp/YEQ8nmMZ6zKzayLTJ0/gqdUDLO1fy6xpk4a7SGZmucutBhERj0TEouz7FcDdwHYlaa6PiCXZ0z8Cs/Mqz6aQhuZCeH9qMxsr1I4mE0lzgauBXSJieYU0xwPPj4hjsucPAkuAAE6LiNMr5JsHzAPo7u7umT9/flNl7O/vp6urq+L5L1+3hJv/sZrj95nJPrOn1EzfzDWGK4/L5XK5XCMnT7vKVdDb27swInrLnoyIXB/AdGAhcFiVNAeSahhbFh3bLvu6DXAr8LJa1+rp6Ylm9fX1VT3/uQvviB1OWBCnXnl/XembucZw5XG5XK4887hcI7NcBUBfVLin5jqKSdJE4DzgnIg4v0KaFwM/BA6NiCcKxyPi4ezrY8AFwJ55lrUWD3U1s7Emz1FMAs4A7o6Ib1RIsz1wPnBURPy56Pi0rGMbSdOAg4E78iprPQZXdfVkOTMbI/IcxbQvcBRwu6RbsmMnAtsDRMSpwGeALYHvpXjCQKS2sG2BC7JjE4CfRcQlOZa1Ji+3YWZjTW4BIiKuBVQjzTHAMWWOPwDsunGO4VM8F2L9es+FMLPRzzOp6zRt8gS2mDaJNQPrefyp1cNdHDOz3DlANMD7QpjZWOIA0YA5XtXVzMYQB4gGzPaifWY2hjQcICTNyuYujDmeC2FmY0ldAULSlZJmSNoCWAT8QFLZuQ2jmedCmNlYUm8NYvNIaygdBpwdEXsBr8yvWCPTHHdSm9kYUm+AmCCpG3gLsCDH8oxo22UB4pFlq1jnuRBmNsrVGyBOBi4F7o+ImyXtCNyXX7FGpskTxrPtjMmsWx88sXLdcBfHzCxX9c6kfiQiBjumI+KBsdgHAamj+tHlq3n0aQcIMxvd6q1BfKfOY6NeoaP6MQcIMxvlqtYgJO0DvBTYWtJxRadmAOPzLNhIVeiodoAws9GuVhPTJNKGPxOAzYqOLwcOz6tQI9ls1yDMbIyoGiAi4irgKklnRcTf2lSmEa0wWc59EGY22tXbST1Z0unA3OI8EfHyPAo1khUW7PuXA4SZjXL1BohfAaeStgYd03fG7s2nMH6ceHLVelatXceUiWOyK8bMxoB6RzENRMT3I+KmiFhYeFTLIGmOpCsk3SXpTknHlkkjSd+WdL+k2yTtXnTuaEn3ZY+jG3xfuZkwfhzPnDkFgIeXeskNMxu96g0Q8yV9UFK3pC0Kjxp5BoCPRcTOwN7AhyTtXJLmVcBzssc84PsA2Wt/FtgL2BP4rKRZdZY1d0PLfnvJDTMbveptYip8gv940bEAdqyUISIeAR7Jvl8h6W5gO+CuomSHktZ2CuCPkmZmS3ocAFweEU8CSLocOAQ4t87y5ioFiCf40DmLmDSh/gVxBwYGmHDRZXWnl8QrdphIT08ThTQz20RK9+acLyLNBa4GdskW/SscXwB8Odu/Gkm/B04gBYgpEfHF7PhJwMqI+FqZ155Hqn3Q3d3dM3/+/KbK2N/fT1dXV11pr3toJd/84zLasRrT5pPFma/ftqE8jbyXZtK3K4/L5XK5XPnlKejt7V0YEb1lT0ZEzQfwznKPOvNOBxYCh5U5twDYr+j574Fe4Hjg00XHTwKOr3Wtnp6eaFZfX19D6a+54aZ44qnVDT3+cF39ef61YlU8+5MXxQ4nLIiVawZyfS+Npm9XHpfL5cozz1gvVwHQFxXuqfU2Me1R9P0U4BWkfSHOrpZJ0kTgPOCciDi/TJKHgTlFz2dnxx4m1SKKj19ZZ1nbYurEcWwxbVJDeWZMbizPM2dO5e9P9rN4ST87bbNZ7QxmZi1UVwN6RPxn0eO9wO6kmkFFkgScAdwdEZUW9rsQeGc2mmlvYFmkvotLgYOz3etmAQdnx8aUOYUtTpd4tJSZtV+9NYhSTwPPqpFmX+Ao4HZJt2THTgS2B4iIU4GLgVcD9wP9wLuzc09K+gJwc5bv85F1WI8ls2emzvDFHi1lZsOgrgAhaT4M9smOB14A/LJankgdz6qRJoAPVTh3JnBmPeUbrVyDMLPhVG8Nonj00ADwt4hYnEN5rMjQHtiuQZhZ+9XbB3EVcA9pRddZwJo8C2XJ7MKEPO+BbWbDoK4AIektwE3Am0n7Ut8oaUwu991Og01MT7qJyczar94mpk8Be0TEYwCStgZ+B/w6r4IZbD19MpPGw7KVa1m+ai0zpkwc7iKZ2RhS7zoR4wrBIfNEA3mtSZLYpiutFrvYtQgza7N6b/KXSLpU0rskvQu4iDRE1XK29bQUINwPYWbtVmtP6p2AbSPi45IOA/bLTt0AnJN34Qy2nTYBWOORTGbWdrX6IP4f8EmAbKmM8wEkvSg797pcS2dsk9UgFnsuhJm1Wa0mpm0j4vbSg9mxubmUyDZQCBCuQZhZu9UKEDOrnJvayoJYedu6BmFmw6RWgOiT9N7Sg5KOIS3hbTnbpqiTOtqwd4eZWUGtPoj/Ai6QdCRDAaEXmAS8Mc+CWTJ90jg2mzKBFasGePLpNWw5ffJwF8nMxoiqASIiHgVeKulAYJfs8EUR8YfcS2aDZs/q4u5HlvPQkpUOEGbWNnXNpI6IK4Arci6LVTBn1tQUIJ7sZ7c51bqFzMxax7OhO8Dgqq6eLGdmbeQA0QHmzEoDxjySyczaqdkd5WqSdCbwWuCxiNilzPmPA0cWleMFwNbZbnJ/BVYA64CBiOjNq5ydwPtCmNlwyLMGcRZwSKWTEfHViNgtInYjzda+qmRb0QOz82M6OMBQgHANwszaKbcAERFXA/XuI30EcG5eZel0281MTUwPL1nJ+vWeC2Fm7aE8J19JmgssKNfEVJSmC1gM7FSoQUh6EFhC2gf7tIg4vUr+ecA8gO7u7p758+c3Vdb+/n66urpyS7+ped594WMsX72e01+zNVtmS4C36jrtfi8j6Roul8s1VstV0Nvbu7BiS01E5PYgrdd0R400bwXmlxzbLvu6DXAr8LJ6rtfT0xPN6uvryzX9puZ5/SnXxg4nLIgbH3ii5ddp93sZSddoJo/L5XLlmadd5SoA+qLCPXUkjGJ6GyXNSxHxcPb1MeACYM9hKNeIUhjJ5I5qM2uXYQ0QkjYH9gd+U3RsmqTNCt8DBwN3DE8JRw53VJtZu+U5zPVc4ABgK0mLgc8CEwEi4tQs2RuByyLi6aKs25LWfyqU72cRcUle5ewUc2Z5spyZtVduASIijqgjzVmk4bDFxx4Ads2nVJ1rzhZuYjKz9hoJfRBWh9mz3MRkZu3lANEhnjlzChI8smwla9etH+7imNkY4ADRISZPGM8zZkxhfcA/lroWYWb5c4DoIHPczGRmbeQA0UFmu6PazNrIAaKDeKirmbWTA0QHmT04m9pNTGaWPweIDuKd5cysnRwgOsjQxkGuQZhZ/hwgOsgzZkxh4njx+FOrWbV23XAXx8xGOQeIDjJ+nHjmzML+1G5mMrN8OUB0mMGRTG5mMrOcOUB0mMFF+1yDMLOcOUB0mNmDNQgHCDPLlwNEh/FcCDNrl9wChKQzJT0mqexucJIOkLRM0i3Z4zNF5w6RdK+k+yV9Iq8ydqLBneWWugZhZvnKswZxFnBIjTTXRMRu2ePzAJLGA98FXgXsDBwhaeccy9lR3EltZu2SW4CIiKuBJ5vIuidwf0Q8EBFrgJ8Dh7a0cB1sq+mTmDpxPMtWrmX5qrXDXRwzG8UUEfm9uDQXWBARu5Q5dwBwHrAY+AdwfETcKelw4JCIOCZLdxSwV0R8uMI15gHzALq7u3vmz5/fVFn7+/vp6urKLX0r8xx76eMsXj7A1w7akmfNnLjJ1xnO9zLc13C5XK6xWq6C3t7ehRHRW/ZkROT2AOYCd1Q4NwOYnn3/auC+7PvDgR8WpTsKOKWe6/X09ESz+vr6ck3fyjzv/tFNscMJC+K3tz/SkusM53sZ7ms0k8flcrnyzNOuchUAfVHhnjpso5giYnlEPJV9fzEwUdJWwMPAnKKks7NjlimMZPJsajPL07AFCEnPkKTs+z2zsjwB3Aw8R9KzJE0C3gZcOFzlHIm8s5yZtcOEvF5Y0rnAAcBWkhYDnwUmAkTEqaSmpA9IGgBWAm/LqjsDkj4MXAqMB86MiDvzKmcnmuOd5cysDXILEBFxRI3zpwCnVDh3MXBxHuUaDWZ7ZzkzawPPpO5AxftCRI6j0MxsbHOA6ECbT53IjCkTWLl2HU88vWa4i2Nmo5QDRIfyon1mljcHiA5V6Kj2SCYzy4sDRIea445qM8uZA0SHKu6oNjPLgwNEhxpqYnINwszy4QDRoea4k9rMcuYA0aEKo5geXrqSdes9F8LMWs8BokNNnTSeraZPYu264NHlq4a7OGY2CjlAdLDZXrTPzHLkANHBhkYyuR/CzFrPAaKDzcn2hfBcCDPLgwNEB/NcCDPLkwNEB/NsajPLkwNEBxvcetR9EGaWg9wChKQzJT0m6Y4K54+UdJuk2yVdL2nXonN/zY7fIqkvrzJ2umfOnIoE/1y+ijUD64e7OGY2yuRZgzgLOKTK+QeB/SPiRcAXgNNLzh8YEbtFRG9O5et4kyaMo3vGFNYHPLLM/RBm1lq5BYiIuBp4ssr56yNiSfb0j8DsvMoyms12R7WZ5UR5blkpaS6wICJ2qZHueOD5EXFM9vxBYAkQwGkRUVq7KM47D5gH0N3d3TN//vymytrf309XV1du6fPK852blnLl31bx/p4ZHLRjV1PXGSnvxeVyuVyu9pWroLe3d2HFlpqIyO0BzAXuqJHmQOBuYMuiY9tlX7cBbgVeVs/1enp6oll9fX25ps8rzzcvvzd2OGFBfOW3dzd9nZHyXobjGs3kcblcrjzztKtcBUBfVLinDusoJkkvBn4IHBoRTxSOR8TD2dfHgAuAPYenhCPf0FBXNzGZWWsNW4CQtD1wPnBURPy56Pg0SZsVvgcOBsqOhLKioa6eC2FmLTYhrxeWdC5wALCVpMXAZ4GJABFxKvAZYEvge5IABiK1g20LXJAdmwD8LCIuyaucnc6zqc0sL7kFiIg4osb5Y4Bjyhx/ANh14xxWzrYzpjBxvHj8qdWsXLOOqZPGD3eRzGyU8EzqDjd+nNhuppuZzKz1HCBGgcFmJgcIM2shB4hRYPYs90OYWes5QIwCc7ZwE5OZtZ4DxCjgGoSZ5cEBYhTwznJmlgcHiFHAe1ObWR4cIEaBLadNYurE8SxfNcCylWuHuzhmNko4QIwCkgY7ql2LMLNWcYAYJQqL9i32on1m1iIOEKOEF+0zs1ZzgBgl3FFtZq3mADFKzPa+EGbWYg4Qo4Q7qc2s1RwgRolCE9PiJSsL27aamW0SB4hRYsaUiWw+dSIr165j2er1w10cMxsFcg0Qks6U9JiksluGKvm2pPsl3SZp96JzR0u6L3scnWc5R4tCM9NjT68b5pKY2WiQ245ymbOAU4CzK5x/FfCc7LEX8H1gL0lbkLYo7QUCWCjpwohYknN5O9rsmV3c8fBy/rZsgCeeWl13vmWr1zeUvl15XC6Xy+WqP8/AuvVMGN/az/zKu71a0lxgQUTsUubcacCVEXFu9vxe0j7WBwAHRMT7yqWrpLe3N/r6+poq58KFC+np6cktfTvyfOmiu/jBNQ829PpmNjr87rj92Wmb6Q3nk7QwInrLncu7BlHLdsBDRc8XZ8cqHd+IpHnAPIDu7m4WLlzYVEH6+/sbytto+nbk2XHiGraZNp6Va9ejBq4R0FD6duVxuVyuPPOMtnLdfdedLHuoxbf0iMj1AcwF7qhwbgGwX9Hz35OalY4HPl10/CTg+FrX6unpiWb19fXlmn4k53G5XK4887hcI7NcBUBfVLinDvcopoeBOUXPZ2fHKh03M7M2Ge4AcSHwzmw0097Asoh4BLgUOFjSLEmzgIOzY2Zm1ia59kFIOpfU4byVpMWkkUkTASLiVOBi4NXA/UA/8O7s3JOSvgDcnL3U5yPiyTzLamZmG8o1QETEETXOB/ChCufOBM7Mo1xmZlbbcDcxmZnZCOUAYWZmZTlAmJlZWQ4QZmZWVu5LbbSTpH8Bf2sy+1bA4zmmH8l5XC6XK888LtfILFfBDhGxddkzlWbQjbUHVWYTtiL9SM7jcrlcLtfIydOuctXzcBOTmZmV5QBhZmZlOUAMOT3n9CM5j8s18q7RTB6Xa+Rdo5k87SpXTaOqk9rMzFrHNQgzMyvLAcLMzMpygDAzs7IcIDaBpG5Jk6ucf66k30u6I3v+Ykmfbl8Jh5ekKcNdhuEgaQtJJ0o6TtKMnK/1rHqOdQJJW+b8+tuXe+R5zU43pjupJe0L3BIRT0t6B7A78K2IqGs2tqTfAc8GzouI48ucvwr4OHBaRLwkO3ZHROzSRFmfERH/rHJ+c+BzwL9lh64i7aOxrNFr1SjHeGBbipaKj4i/V0h7P/AocE32uLZWeSQJOBLYMSI+n/0DPyMibqqQ/p3ljkfE2VWusUWZwysiYm2VPAtJy8//LCKW1HgPVwA3AJOBQ4DXRcQDNfKMB34XEQdWS1cm36KI2L20rBHR08jr1LjG+cAZwG8jYn2debYF/ht4ZkS8StLOwD4RcUaVPPcBtwA/yq5V181J0n7AcyLiR5K2BqZHxINl0t3O0JbPU4BnAfdGxAsrvO5h1a4bEedXKdPvI+IVtY41k77ofVQq14urlbsRue4H0QG+D+wqaVfgY8APgbOB/evJHBGvzG5oO1dI0hURN6UkgwaaLOsZwGuqnD8TuAN4S/b8KNI/WtU/8mLlbjYl5/+TtOnTo0DhRhFA2T/IiNgpu8H/W1b270paGhG7VSnG97LXfjnweWAFcB6wR4X0xcenAK8AFpF+j5UsIm1pu4R0s5gJ/FPSo8B7I2JhmTxvJW1odbOkPtLP9rIKN7EtI+JEAEmXAVdJWkr6GzsmIt5SmiEi1klaL2nzeoK6pOcDLwQ2L7mRzSD9HMrlWUH1G0ul2s73SO/925J+BfwoIu6tUcSzSD+jT2XP/wz8gvR3XMlzgVcC78mu9UvgrIj4c6UMkj5L2sf+edn1JgI/BfYtTRsRLyrJuzvwwSrleV32dRvgpcAfsucHAtcDGwWIrNbcRdokbRbp7wvS72W7TU2feW32tbCXzk+yr0dWeS/NyWN6dqc8gEXZ188A/1F8rEWv/1tSDaNwncNJn4zyeC+31HMsOz4eOKeJa9xPuvnVm342cARwKukT9UXAJ+v8nfyp6NitDVxzJnBJjTQ/AP696PnBwGnA3sCNNfKOA15P2iP978DJwBYlaa4D5hY9F+mfvQvorvLav8le8wzg24VHhbSHkm6IT2RfC49vAy+t8R6+QLoxbka6EX2AVNus9bPdHHg/8BDpBvluYGKFtDeX+T2W/XuskP/A7Ge8lFQb3qfS33328y2+zm0NXOf2OtJcVvx7A7qBSyukPRZ4EFgNPJB9/yBwK/DhTU1fkvdPZY617P4VEWM+QFwFfJL06eYZ2T9/zT+YBl5/R+B3pO1UHwauLb5xtPi93ADsV/R8X+CGKumvBSY1eI0rgAkNpF8P3Agc2kCeG0kBrBAoti73j1Al/0RSs0G1NBv9jgs3lWo3MVJN6ZvAvdmNeHtua0QAABg/SURBVC9SreCWknTPA57bxO/w6HKPGnnK3jhr5Nko4JY7VnJ+y+xm1kfaS/6twHeAKyukvzLLU/g97g1c1cA1LiLVfieQaggPVshzU/a1cJ1pVAgQwHFFj+OBn1HhRl+S7+6S5+NKj5XJ858N/k4aSl/4WwX2LXr+0mp/v808xnoT01uBt5NqD//MmkO+2qoXj9Tu/EpJ04BxEbGiVa9dxgeAH2d9EQKeBN5VJf0DwHWSLgSeLhyMiG+UJpR0XFGeKyVdRPrEUzFP5iXAfsDbJX0CuI90k6jWzPBt4AJgG0lfItW6KnbsS5rPULPJeOAFwC+rvD7AI5JOAH6ePX8r8GjWD1C2jT3rg1hK+nT/iYgovP8bs76sQVG7+aWsiPixpKnA9g28xp8kfYjU3DTYtBQR76mS52lJR5Lef5BqeU9XSizpAlLQ+wmpP+WR7NQvsua2co4jBZJnS7qOFOgPr/Febsiu8YaIWFx0vE/SqRXy/FLSacBMSe8lNU/9oELazYq+HyAFofNqlAng95IuBc7Nnr+V9MFvI5Jeln17cx2vOygiviNpF1JzdfHvsVpT6XuAH2X/85D+Pqv93hs2pjup85aNcHoTMJcNO3U/n+M1Z2TXWF4j3WfLHY+Ik+tNO5Sl8vuRNJ0UJP4NeEeWYYcKaceRPmk+SepLEPD7iLi7yusX9xcNAH8rubmUy7MVqS9lv+zQdaSmomWkm/P9ZfLsGDU6mjeVpNcBXyPV7J4laTdS08/rq+T5FXAP6YPO50nt0HdHxLFV8swFvkWqZQbp/f9XRPy1QvpXR8TFJccmFwXJSteZQAosItXqKg4CyNLPi4jTS459OSI+USPfQaRmQpFqBJfXSD8dICKeqpauJM9hDA0AuToiLqiQ7kfZt0sj4qMNvP5ngQNIAeJi4FWkQR1lg2r2YeYjEfHNQoCIFg9IgTEaICRdGxH7lem0E+mG15KhiZIuId10FgLrCscj4uuteP3sGsdVO1/l030z13pzRPyq1rGic32kkTzXk41kihojxCT9KbIRXw2Ua1uGOqtviojHGsnfwHVew8af1FsW7LNaystJzTZ1jXor/Lwk3RYRL5Y0kfRz3ruF5So3UqrWgIbxpIEJc9nww1HFv0dJF5P6xs7Jnn8XmBIR/7Fp72Dw9Xch1VAKo9geJzXh3dGK1y+51hRSE9lc6vhwmI1M2pXUnLpr9jf904g4qMo1boqIPVta8BJjsokpIvbLvm5WK+0mmh0Rh+R8jYbeQ9akVFG1T6uk/prSYFDuWMGrIuJfDRQPUnX+TcD5UcenF0lvITULXkkK8N+R9PGI+HWVPM8ltUHPZcN/3pdXyXMqqZP5QNJot8OBskNvN8HaiFhWMuqt1rDSwqfypdkN8J+kUTcVZUNB38vG7/89JemeQepcnyrpJWw4wqarRrnmA6uA2+t4DwVvAi6UtJ40PHhpreBQ8iFvEqkP6ukKH/JOB46LiCuyvAdkx15a47VF4x8k/5ehD4dVa1qZlRGxXtJA1grwGGmkXTXXSTqFNDqsuJl4UR3Xq8uYDBBtdL2kF0XE7XldoFyTUA37kEahnEvqEFb15CDpVcCrge0kfbvo1AyqD9tdI+kbQKFdtp65Ge8jtV8PSFpF7X/GTwF7FGoN2c3vd0DFAEEKaKeSbvTrqqQr9tLsE/ptEXGypK+TRqm10p2S3g6Ml/Qc4COk2lc1p2fDI08itflPJ43Kq+Y3pBrd76j+/v+d1I81Gyj+5L8COLHGNWZHnePxteG8lGNIN9frgJMlbRERT1bKW/whTymyHkpqpixnWiE4ZHmvzPoHa752Exr9cNgnaSYpYC0EnqL2774wXLy4VhKkWmhLjMkmprwpzZxeTwrAzyF17q5m6GbXsoksJTfsjUTER0rSjwcOInVMvpjUUXduRNxZ5Rq7kv4YP8+GN58VwBVRYeKYpPNIczN+nB06Ctg1IqrOzchuGM9hw6acqyqkvT2Kxrdn/Ri3RsmY95I8DU8kk3RjROwl6Y+kpoMngDsjYqdGXqfGNbpIAW+wPR34QkSsatU1suvcEtXnopSmf1NE1NOZW5znK6T+o8vqSPsgG39Cp3AsInZs8NobNFNmTVcfIgW5RQzNG3gH0BMRb6zxemVnW0eFCaJZntOB79T74TD7u307aeTj2cD2wKqoMEG0XRwgciBpCUPRfSO12uEbvNbR1c5HxI8rncs60Y8gNdGcHBGn1LjWhIioe6JfuRtRrZuTpGNIQx1nk4bx7Q1cH5VnoP4Pqe22eITJbRFxQpVrfI5Uhb+ADUdjVfykKukk0rDOVwDfJd28fhgRJ1XK0w5qbsbyF0k/04srpcnSvSMifirpY5SZYFejP+GNpAlr40jNYDWbZbLmwksiYnn2896dFCArNplow0mC40hDYvePiH2K0rwZ+BIpMHSThidDqkV9LiKWVnr9LH/xTb7mDOwsz13ATgzNcaj64VDS98kmiEbEC7Ja4WURUWmCaCFfvv1iDhCtV6sDL+drd0VEf400k0kdiEeQ2qEvBM6MiIcrpP9lRLxFFab4V/mjvwH4eERcmz3fF/ha8T9vmTy3kzqc/xgRuynNGP7vSrUOSR8hNZkVRphcExVGmBTlebDM4aj3k2r285vS6lEjTfaN/JZsxnLWuTmB1NFZrQa1gjRfYDVVbt6S3hcRp6n8KLaodiPKfsaHkuac1HWTKepo3480me9rwGciYq8qeX5U9HQA+CvwgygZqKA0cukkUt/GTxj6O45qga7CNXcHPhgRx1RJU3akXqUPh4V7RnHtR9KtEbFrlWuU7RdrVac+uA8iL9uoyuiiRv8g6yFpH9IY/enA9lmz0Psi4oMl6c4GdiENpTu5zhEchSGTr62aamPFczMgLW1RtcZDqlavklQYSnmPpOdVSb8Nqa1+EWm5kUtrFSoi6l7MTlXW45FEVFmPpwnN9I1sFRG/lPRJgIgYkFQ1b0RsVq4Zr0y607JvdwSOLXzSzj7d1hqJ9xBwR73BIVMo92tIN/mLstpORRHx7jpfew2pI3cy6X+k6U/GEbFIUsWglaVptJVgbdb8m6bep760Wp37ufeLOUDkYzzpj7BmB3AL/T9Sp+KFABFxq4Ym7RR7B+kf5VjgIxoaMVOxCSCyiVGFP/pslEU9fzt3A/9DWm5kJmlUxxuA26rkWZx11v0vcHnWXFfxny0iPp01RxxMWvrhFKU1fM6IiL8Up5X08oj4Q6WbfoWbfcPr8WyCgYj4foN5nlZaBbVwY9mb9HOuqFIzHqn5rJwXFzfDRMQSpVFN1RQmVf6W+iZVAjysNOntIOArWU2t7IrTkr5D9XWlPlKU9hBS/8OFwO61athlrlX8YW8cqenrH428Rh0amiCaWZl97Zf0TFK/WHcrC+UAkY9HWtkOWK+IeEgbDpHc6JNkRDS9xLuk95EmlK2iqIpO+oRZzm9IszsXkZYaqamow/BzSquibg5cUiNPSPonaYjnADAL+LWkyyPi/xQl3Z90g39duZehzM2+8AlVadG9nQvBUlI3aUG6TaahUTzzJX2QBvpGaG7G8rEMNeMdWGjGq5J+nKRZhcEIWXlr3TsKawpNyh71eAupCehrEbE0+xl/vELaSjO4y/kU8OaoMhCjhmZnYNctIs5RmgdTmCD6hqgyQTSzIPsw9VXS/1hQeRZ5U9wHkYPSURRtuuavSZ+STiF1wh0L9EbE21p4jftIHaCP15m+qaXNGyzTscA7SZOefgj8b0SszUaF3BcRz27Rde6OiBcUPR9HGsX0girZ6n3twiiecjXOin0jWZPER0id543MWL45IvaQdAuwV0SslnRnpU5XpSXVT2RovsubgS9FxE/KpS/J2/Cs5WZktdqIHJezadd7aUZe/WKuQeSjUlU9T+8nLZ+wHenT+mUMLQfcKn8hLTxYr9zngZBmxR5W2uYbadJR2T6TrE/kszQ2P6Pu9XgaVegTkTQlSoa0qsqmS5GWCD8iIr4JNPLpuNFmvLOVZsUXOssPi4i7ql1AJbOWJT0OvHMTPsVXuk4vqZN+s/RUS4H3RPkl25u9Rrn3cnSd/Xe5UZo1/wGG/o6vlHRarQ8IDV3DNQirV9bu/CPSBLviJpDSuRaF0U65zwNphpqfn/FGhv4ZK67HswnlamZJi2+SZg83NZtWaS2rzUnDS9c0VfDyr3s9aWRV8azl/46IsrOWN+E6twEfiohrsuf7Ad9r5d9Yu95LE+X6Iel3X/x3vK7a6KpGuQbR4RrprGuB00ht+LWWT2h0tFO7PTsi3lT0/OSsuaWW60lt0EELl9nQpi1pUZhTUphRX1gWoq7ZtFFhAmILNDRreROsKwSH7DrXSmp2U65K2vVeGrVHyTDYP0i6tZUXcIDofMWddSeTmk7yMjEiqi4OCK2dCJiTlZL2iw3nZ6yslkFNrPnUgE1Z0mIBG/ZfBLBc0m4RUU/Qy8sD2eiy4lnLLVsNV2kuAqTd+k4jNf0FqenvylZdJ5Pre9kE6yQ9uzBaT9KO1D88ui5uYhpF8u4cl/TfpIlI86l/lM2IozRH5GxS0wpk8zMiouLw2+yT2UFRsuZTtYlMTZSrmSUtfkaaPXwhKUi8ljSMeC7wq4j4n1aVr8FyzSJ9YCksqX41ad5N1f28G3j9K6qcjqgyubCJa5W+l8IM7Ja8l2ZJegWpyfcB0u9+B+DdxbWdTb6GA8ToUau9ugWvX5iBvMEfTaVRNiNNyXh2kWYTQ2q7j2pj9NXEmk9NlrGhpRMkXQ28ujCyJhtpcxFpuOjCiKi0X3rbZKOtpkWNPUqscdnopcJE0nujxh4djXITk9UkaQ/goaLRNkeTlmb+K/C54StZwwrj2Z9HmgfwG1KgeAe1+xR+W2YUU9W1jBql5pYU34YNl5NeC2wbESsltfRm0YisZvN+UpPHzcAMSd+KiJbt2FjmmgsiouX9X2piCZR2yEa4fZBUswngGkmnlo6E26RruAbR2bThevhdDA1DbdnmR5IWAa+MiCeVZmf/HPhPUgfpC6LCrlcjVfap+zWFMfOSNgMuiohyM88Leb5CGr1V3Mywd1RZFLCJchXWIip8nQ78NiL+rUqek4A3koIdpEmAF5KWwjg9Io5sVfkaoWxRRqWtTXcHPkGq0eQ2gi2vJtasefFUNt74q2VDaZuhtGLACtKiiJBWg50ZEW9u1TVcg+hwkf+mRwDji/oZ3kq68ZwHnFfn6J+RZlvS2jwFa7Jj1RyUBYPB2daSTgZaFiBoYumEiPiC0nIWhX2x3x8RhYELwxIcMhOzcfpvAE7JJi/m/Wn0Tzm9bjNLoLTDLiVNiFcorSLbMg4QVo/xGlrq+xXAvKJznfg3dDZwk6TCPIY3UGHZDEkfIFXjd8zG3BdsRtrUppXKLZ3ww1qZsoDQyNIT7XAaqQnyVuBqpdVNc+2DiJId8TaVNm0JlHZYJGnviPgjgNICgi39O3ATk9Uk6VOkHeUeJ21ksntEhKSdgB9HxL5VX2AEyoZJFm9CX/bTZzbrehbwf0nNJAUr8rxB5LV0wnBSg/uJ1Pma+5L6wXYgfVgpNK1u8sAJlV8CZfCGOVyDM4omok4k9af9PXu+A3BPKwcmOEBYXZRWCe0mbWLydHbsucD0emftWnVKO8p9DNg+It6rtO3o8yJiwTAXrWFqYiOjJq9zD/BRNu4feKKF12h4I6M8qcJeEwWtnIfkAGE2Qkj6BelG986I2CULGNdHA9uDjhRqYiOjJq9zY1TZUKhF12h4I6Ocy7NFtfOtrNk2vfSzmbXcs7OJbWsBIu1b0M49RVppq4j4JdmSLFnTUstm+UraPWsmvELSVyXtUzhWNMu6VTbayIj6lzDPw0JSX8NC4F/An4H7su9bOrKqEzsYzUarNZKmMrT5z7PZcI5DJ2l4I6MGle5o11v0fd1rUdWp7o2M2qFoPtIPgAsi21tc0qtIAy5axk1MZiOEpINIu4jtTFqufV/gXRFx5XCWqxmSeki7pO1CWjl3a+DwasuZjFRZU98hpP2171PayOhFEXHZMJfr9tImu3LHNukaDhBmI4Okn5LWUVpJWl/nxqhzc6aRKOt3qHsjoyavcSypr2MFaTe13YFPDPfNux2ymf3XMDRR7kjgZRHx7626hvsgzEaOM0hrML2etEvcadkNsONIupa0wN0c4G95BIfMe7I1ng4GtiTtifDlnK410hxBqpldQJrAuXV2rGVcgzAbQbKF7fYgrcf0fmBlRDx/eEvVOEnPIs0z+Tdgb1JfyjUR8dEWX6cwwuhbwJURcUHeqxqPBNnfydl5L6XiTmqzEULS70krzN5AajrYo7C8eKeJiAclrSItY7KGFPA2ef/uMhZKugx4FvDJbF2taptZjQqRtpvdQdKkaOFOgKVcgzAbIZS2D+0hfdq+jrSHwg0RUXUzo5FI0l9IM+9/Rgp2t0REy2/c2bLruwEPRMTSbOTUdp3YGd4oSWeTgu6FbLjdbMVl6xvlGoTZCFFofsk+Bb+L1Pn6DGDyMBarWd8mrXx7BPAS0s5vV0e2+9mmkvT8iLiHoS1Xd5Q6dcpI0/6SPcYxtJR9S7kGYTZCSPowqc2+h7TQ3TWkdvs/DGe5NkW2ZPm7SfspzI6I8S163dMjYp7K7ywXw71Xw2jhAGE2Qkg6nhQUFrZ6Ubt2k/R1UrCbBlwPXEsKdi3dy1nSuNKmK0lTWrlpzkiltO3t/2HjHQhbFhw9zNVshIiIr0XEjZ0eHDI3AIcC7wF+AjwIzM7hOhsshy5pGmnL1bHgHOAeUgf9yaRa582tvIADhJnlYRZpNvglpJvXpeSzPe3Dkr4HIGkWcDlDE8dGuy2z1XHXRsRV2X4YLW1ac4Awszx8hDSf428RcSCpo3ppqy8SEScBT2X7eV8GfD0iftTq64xQhcmHj0h6jaSXAFVXem2URzGZWR5WRcQqSUiaHBH3SHpeq15c0mFFT28ETgJuAkLSYRFxfvmco8oXsw2tPkaaeT8D+K9WXsABwszysDjbPvV/gcslLQFatpEN8LqS538i7bD2OtJqrmMhQLwZuDYi7gAOzPaJ+Bowv1UX8CgmM8uVpP2BzUm7suU263esKbekSKuXGXENwsxyFRFX5fXa2VDP9wJzKbqfZR22o904SbMiYgkM7jTX0nu6A4SZdbLfkOaO/I4W7ljXIb4O3CDpV9nzNwNfauUF3MRkZh1L0i2duGd3q0jamaGhrX+IiLta+voOEGbWqSR9Ebi+sO2mtZYDhJl1LEkrSMt5rCbNCxBpLaYZw1qwUcIBwszMynIntZl1nMJy35J2L3c+Iha1u0yjkWsQZtZxSpb7Lr6JFZqYvNx3CzhAmFnHkjQV+CBpc6IgDXn9/lhY7rsdHCDMrGNJ+iWwnLT0NcDbgc0j4i3DV6rRwwHCzDqWpLsiYudax6w5Xu7bzDrZIkl7F55I2gvoG8byjCquQZhZx5J0N/A84O/Zoe2Be4EBUmf1i4erbKOBA4SZdSxJO1Q7HxGtXGJ8zHGAMDOzstwHYWZmZTlAmJlZWQ4QZmVI+pSkOyXdJumWbHRMXte6UlJvXq9v1iyvxWRWQtI+wGuB3SNitaStgEnDXCyztnMNwmxj3cDjEbEaICIej4h/SPqMpJsl3SHpdEmCwRrANyX1Sbpb0h6Szpd0X7ZfAZLmSrpH0jlZml9L6iq9sKSDJd0gaZGkX0manh3/sqS7shrN19r4s7AxzAHCbGOXAXMk/VnS9yTtnx0/JSL2iIhdgKmkWkbBmojoBU4lbYP5IWAX4F2StszSPA/4XkS8gLQ8xAeLL5rVVD4NvDIididN+Douy/9G4IXZuP4v5vCezTbiAGFWIiKeAnqAecC/gF9IehdwoKQbJd1O2ubxhUXZLsy+3g7cGRGPZDWQB4A52bmHIuK67PufkhaYK7Y3sDNwnaRbgKOBHYBlwCrgDEmHAf0te7NmVbgPwqyMiFgHXAlcmQWE9wEvBnoj4iFJnwOmFGVZnX1dX/R94Xnh/6x00lHpcwGXR8QRpeWRtCfwCuBw4MMM7UNslhvXIMxKSHqepOcUHdqNtHwDwONZv8DhTbz09lkHOKRVR68tOf9HYF9JO2XlmCbpudn1Ns/2Xf4osGsT1zZrmGsQZhubDnxH0kzSmj73k5qblgJ3AP8Ebm7ide8FPiTpTOAu4PvFJyPiX1lT1rmSJmeHPw2sAH4jaQqplnFcE9c2a5iX2jBrA0lzgQVZB7dZR3ATk5mZleUahJmZleUahJmZleUAYWZmZTlAmJlZWQ4QZmZWlgOEmZmV9f8B7XF/cSOIfdsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'needn', 'whom', 'how', 'out', 'here', 'them', 'are', 'above', 'up', 'all', \"couldn't\", 'why', 'his', 'a', 'they', 'wouldn', 'i', 'than', 'couldn', \"doesn't\", 'he', 'our', 'ain', 'wasn', 'too', 'having', 're', 'ours', 'himself', 'your', 'yourself', 'own', 'any', 'for', 'because', 'doing', 's', 'an', 'now', 'aren', 'at', 'doesn', 'about', 'when', 'we', 'each', 'hers', 'didn', \"you're\", \"aren't\", 'below', 'on', 'what', 'has', 'have', 've', 'it', 'y', 'was', 'him', \"shouldn't\", 'hasn', 'yourselves', 'again', 'nor', 'theirs', 'did', 'under', 't', 'the', 'yours', 'me', 'is', 'in', 'such', \"wasn't\", 'o', 'will', \"needn't\", 'won', 'by', 'my', 'off', 'once', \"you'll\", 'be', 'ourselves', \"she's\", 'ma', 'that', 'were', \"it's\", \"hasn't\", 'isn', \"won't\", 'other', 'do', 'shan', \"that'll\", 'down', \"didn't\", 'itself', 'most', 'can', 'between', \"isn't\", 'themselves', 'who', 'as', 'from', 'or', 'over', 'of', 'which', 'does', 'been', 'myself', 'these', 'after', \"mustn't\", 'few', 'd', \"mightn't\", 'don', 'very', 'just', 'should', 'there', 'no', 'you', \"weren't\", 'through', 'to', 'further', 'while', 'both', \"should've\", 'shouldn', \"don't\", \"hadn't\", 'with', 'against', 'if', 'before', 'into', 'same', 'so', 'she', 'her', \"haven't\", 'and', 'those', 'll', \"you've\", 'until', 'this', 'then', 'haven', 'weren', 'being', 'during', 'hadn', 'mightn', 'mustn', \"you'd\", 'only', \"shan't\", 'had', 'am', 'herself', 'but', 'some', 'm', 'more', 'where', \"wouldn't\", 'not', 'their', 'its'}\n",
            "Filtered Sentence: []\n",
            "Stemmed Sentence: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IynmsTkGMfsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e8101c-835a-4902-97b5-f066bde4be01"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sent = \"আঁই ন গইজ্জুম|\"\n",
        "tokens=nltk.word_tokenize(sent)\n",
        "print(tokens)\n",
        "nltk.pos_tag(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['আঁই', 'ন', 'গইজ্জুম|']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('আঁই', 'JJ'), ('ন', 'NNP'), ('গইজ্জুম|', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0JZln4V5jf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1c2852-63d2-4bc0-fd9d-cfd330b82dd7"
      },
      "source": [
        " import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0saePK2PRqAZ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_counts, data['Sentiment'], test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDgF4VPJNId-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ccb0763-e7b3-4abe-97da-c9ffd6e81ab6"
      },
      "source": [
        " import nltk\n",
        " nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RzB0NWSL2hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f120ba1-8e09-453b-98e3-6b71bab6fe1c"
      },
      "source": [
        "  import nltk\n",
        "  nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8zOMP7AJhOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ab751a-2459-4fc2-d4fa-257b135cc417"
      },
      "source": [
        "  import nltk\n",
        "  nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23H_QQYVHjmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46225340-6748-4ebe-a68d-d49c215d2c74"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpGz12JUq-Y9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snqRnrIfII4S",
        "outputId": "f863be27-4be8-412a-9f8e-6c3bc53a6b98"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[36, 27], [30, 8], [49, 30], [27, 8], [14], [34], [37, 30], [32, 30], [37, 8], [48, 25, 27, 1]]\n",
            "[[36 27  0  0]\n",
            " [30  8  0  0]\n",
            " [49 30  0  0]\n",
            " [27  8  0  0]\n",
            " [14  0  0  0]\n",
            " [34  0  0  0]\n",
            " [37 30  0  0]\n",
            " [32 30  0  0]\n",
            " [37  8  0  0]\n",
            " [48 25 27  1]]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 89.999998\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}